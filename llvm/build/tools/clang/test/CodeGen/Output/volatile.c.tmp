; ModuleID = '-'
target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%struct.anon = type { i32 }
%struct.anon.0 = type { i32 }
%struct.anon.1 = type { i32 }
%struct.anon.2 = type { i32 }
%struct.anon.3 = type { i32 }
%struct.anon.4 = type { %struct.anon.5 }
%struct.anon.5 = type { i32 }
%struct.anon.6 = type { %struct.anon.7 }
%struct.anon.7 = type { i32 }
%struct.anon.8 = type { i8, [3 x i8] }
%struct.anon.9 = type { i8, [3 x i8] }
%struct.anon.10 = type { i32 }

@S = common global i32 0, align 4
@vS = common global i32 0, align 4
@pS = common global i32* null, align 8
@pvS = common global i32* null, align 8
@A = common global [10 x i32] zeroinitializer, align 16
@vA = common global [10 x i32] zeroinitializer, align 16
@F = common global %struct.anon zeroinitializer, align 4
@vF = common global %struct.anon.0 zeroinitializer, align 4
@F2 = common global %struct.anon.1 zeroinitializer, align 4
@vF2 = common global %struct.anon.2 zeroinitializer, align 4
@vpF2 = common global %struct.anon.3* null, align 8
@F3 = common global %struct.anon.4 zeroinitializer, align 4
@vF3 = common global %struct.anon.6 zeroinitializer, align 4
@BF = common global %struct.anon.8 zeroinitializer, align 4
@vBF = common global %struct.anon.9 zeroinitializer, align 4
@V = common global <4 x i32> zeroinitializer, align 16
@vV = common global <4 x i32> zeroinitializer, align 16
@VE = common global <4 x i32> zeroinitializer, align 16
@vVE = common global <4 x i32> zeroinitializer, align 16
@vtS = common global i32 0, align 4

define i32 @main() nounwind {
entry:
  %i = alloca i32, align 4
  %coerce = alloca %struct.anon.10, align 4
  %agg.tmp.ensured = alloca %struct.anon.2, align 4
  %agg.tmp.ensured14 = alloca %struct.anon.2, align 4
  %0 = load i32* @S, align 4
  store i32 %0, i32* %i, align 4
  %1 = load volatile i32* @vS, align 4
  store i32 %1, i32* %i, align 4
  %2 = load i32** @pS, align 8
  %3 = load i32* %2, align 4
  store i32 %3, i32* %i, align 4
  %4 = load i32** @pvS, align 8
  %5 = load volatile i32* %4, align 4
  store i32 %5, i32* %i, align 4
  %6 = load i32* getelementptr inbounds ([10 x i32]* @A, i32 0, i64 2), align 4
  store i32 %6, i32* %i, align 4
  %7 = load volatile i32* getelementptr inbounds ([10 x i32]* @vA, i32 0, i64 2), align 4
  store i32 %7, i32* %i, align 4
  %8 = load i32* getelementptr inbounds (%struct.anon* @F, i32 0, i32 0), align 4
  store i32 %8, i32* %i, align 4
  %9 = load volatile i32* getelementptr inbounds (%struct.anon.0* @vF, i32 0, i32 0), align 4
  store i32 %9, i32* %i, align 4
  %10 = load i32* getelementptr inbounds (%struct.anon.1* @F2, i32 0, i32 0), align 4
  store i32 %10, i32* %i, align 4
  %11 = load volatile i32* getelementptr inbounds (%struct.anon.2* @vF2, i32 0, i32 0), align 4
  store i32 %11, i32* %i, align 4
  %12 = load %struct.anon.3** @vpF2, align 8
  %x = getelementptr inbounds %struct.anon.3* %12, i32 0, i32 0
  %13 = load volatile i32* %x, align 4
  store i32 %13, i32* %i, align 4
  %14 = load i32* getelementptr inbounds (%struct.anon.4* @F3, i32 0, i32 0, i32 0), align 4
  store i32 %14, i32* %i, align 4
  %15 = load volatile i32* getelementptr inbounds (%struct.anon.6* @vF3, i32 0, i32 0, i32 0), align 4
  store i32 %15, i32* %i, align 4
  %16 = load i32* bitcast (%struct.anon.8* @BF to i32*), align 4
  %bf.clear = and i32 %16, 7
  %17 = shl i32 %bf.clear, 29
  %bf.val.sext = ashr i32 %17, 29
  store i32 %bf.val.sext, i32* %i, align 4
  %18 = load volatile i32* bitcast (%struct.anon.9* @vBF to i32*), align 4
  %bf.clear1 = and i32 %18, 7
  %19 = shl i32 %bf.clear1, 29
  %bf.val.sext2 = ashr i32 %19, 29
  store i32 %bf.val.sext2, i32* %i, align 4
  %20 = load <4 x i32>* @V, align 16
  %vecext = extractelement <4 x i32> %20, i32 3
  store i32 %vecext, i32* %i, align 4
  %21 = load volatile <4 x i32>* @vV, align 16
  %vecext3 = extractelement <4 x i32> %21, i32 3
  store i32 %vecext3, i32* %i, align 4
  %22 = load <4 x i32>* @VE, align 16
  %23 = shufflevector <4 x i32> %22, <4 x i32> undef, <2 x i32> <i32 1, i32 0>
  %vecext4 = extractelement <2 x i32> %23, i32 1
  store i32 %vecext4, i32* %i, align 4
  %24 = load volatile <4 x i32>* @vVE, align 16
  %25 = shufflevector <4 x i32> %24, <4 x i32> undef, <2 x i32> <i32 2, i32 1>
  %vecext5 = extractelement <2 x i32> %25, i32 1
  store i32 %vecext5, i32* %i, align 4
  %call = call i32 @aggFct()
  %coerce.dive = getelementptr %struct.anon.10* %coerce, i32 0, i32 0
  store i32 %call, i32* %coerce.dive
  %x6 = getelementptr inbounds %struct.anon.10* %coerce, i32 0, i32 0
  %26 = load i32* %x6, align 4
  store i32 %26, i32* %i, align 4
  %27 = load volatile i32* @vtS, align 4
  store i32 %27, i32* %i, align 4
  %28 = load i32* %i, align 4
  store i32 %28, i32* @S, align 4
  %29 = load i32* %i, align 4
  store volatile i32 %29, i32* @vS, align 4
  %30 = load i32* %i, align 4
  %31 = load i32** @pS, align 8
  store i32 %30, i32* %31, align 4
  %32 = load i32* %i, align 4
  %33 = load i32** @pvS, align 8
  store volatile i32 %32, i32* %33, align 4
  %34 = load i32* %i, align 4
  store i32 %34, i32* getelementptr inbounds ([10 x i32]* @A, i32 0, i64 2), align 4
  %35 = load i32* %i, align 4
  store volatile i32 %35, i32* getelementptr inbounds ([10 x i32]* @vA, i32 0, i64 2), align 4
  %36 = load i32* %i, align 4
  store i32 %36, i32* getelementptr inbounds (%struct.anon* @F, i32 0, i32 0), align 4
  %37 = load i32* %i, align 4
  store volatile i32 %37, i32* getelementptr inbounds (%struct.anon.0* @vF, i32 0, i32 0), align 4
  %38 = load i32* %i, align 4
  store i32 %38, i32* getelementptr inbounds (%struct.anon.1* @F2, i32 0, i32 0), align 4
  %39 = load i32* %i, align 4
  store volatile i32 %39, i32* getelementptr inbounds (%struct.anon.2* @vF2, i32 0, i32 0), align 4
  %40 = load i32* %i, align 4
  %41 = load %struct.anon.3** @vpF2, align 8
  %x7 = getelementptr inbounds %struct.anon.3* %41, i32 0, i32 0
  store volatile i32 %40, i32* %x7, align 4
  %42 = load i32* %i, align 4
  store volatile i32 %42, i32* getelementptr inbounds (%struct.anon.6* @vF3, i32 0, i32 0, i32 0), align 4
  %43 = load i32* %i, align 4
  %bf.value = and i32 %43, 7
  %44 = shl i32 %bf.value, 29
  %bf.reload.sext = ashr i32 %44, 29
  %45 = and i32 %bf.value, 7
  %46 = load i32* bitcast (%struct.anon.8* @BF to i32*), align 4
  %47 = and i32 %46, -8
  %48 = or i32 %47, %45
  store i32 %48, i32* bitcast (%struct.anon.8* @BF to i32*), align 4
  %49 = load i32* %i, align 4
  %bf.value8 = and i32 %49, 7
  %50 = shl i32 %bf.value8, 29
  %bf.reload.sext9 = ashr i32 %50, 29
  %51 = and i32 %bf.value8, 7
  %52 = load volatile i32* bitcast (%struct.anon.9* @vBF to i32*), align 4
  %53 = and i32 %52, -8
  %54 = or i32 %53, %51
  store volatile i32 %54, i32* bitcast (%struct.anon.9* @vBF to i32*), align 4
  %55 = load i32* %i, align 4
  %56 = load <4 x i32>* @V, align 16
  %vecins = insertelement <4 x i32> %56, i32 %55, i32 3
  store <4 x i32> %vecins, <4 x i32>* @V, align 16
  %57 = load i32* %i, align 4
  %58 = load volatile <4 x i32>* @vV, align 16
  %vecins10 = insertelement <4 x i32> %58, i32 %57, i32 3
  store volatile <4 x i32> %vecins10, <4 x i32>* @vV, align 16
  %59 = load i32* %i, align 4
  store volatile i32 %59, i32* @vtS, align 4
  %60 = load i32* @S, align 4
  %inc = add nsw i32 %60, 1
  store i32 %inc, i32* @S, align 4
  %61 = load volatile i32* @vS, align 4
  %inc11 = add nsw i32 %61, 1
  store volatile i32 %inc11, i32* @vS, align 4
  %62 = load i32* @S, align 4
  %63 = load i32* %i, align 4
  %add = add nsw i32 %63, %62
  store i32 %add, i32* %i, align 4
  %64 = load volatile i32* @vS, align 4
  %65 = load i32* %i, align 4
  %add12 = add nsw i32 %65, %64
  store i32 %add12, i32* %i, align 4
  %66 = load volatile i32* @vtS, align 4
  %inc13 = add nsw i32 %66, 1
  store volatile i32 %inc13, i32* @vtS, align 4
  %67 = bitcast %struct.anon.2* %agg.tmp.ensured to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %67, i8* bitcast (%struct.anon.2* @vF2 to i8*), i64 4, i32 4, i1 true)
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* bitcast (%struct.anon.2* @vF2 to i8*), i8* bitcast (%struct.anon.2* @vF2 to i8*), i64 4, i32 4, i1 true)
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* bitcast (%struct.anon.2* @vF2 to i8*), i8* bitcast (%struct.anon.2* @vF2 to i8*), i64 4, i32 4, i1 true)
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* bitcast (%struct.anon.2* @vF2 to i8*), i8* bitcast (%struct.anon.2* @vF2 to i8*), i64 4, i32 4, i1 true)
  %68 = bitcast %struct.anon.2* %agg.tmp.ensured14 to i8*
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %68, i8* bitcast (%struct.anon.2* @vF2 to i8*), i64 4, i32 4, i1 true)
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* bitcast (%struct.anon.2* @vF2 to i8*), i8* bitcast (%struct.anon.2* @vF2 to i8*), i64 4, i32 4, i1 true)
  ret i32 0
}

declare i32 @aggFct()

declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture, i8* nocapture, i64, i32, i1) nounwind
