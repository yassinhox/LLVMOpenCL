; ModuleID = '/home/jazouani/llvm/llvm-3.2/tools/clang/test/CodeGen/builtins-x86.c'
target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-unknown"

define void @f0() nounwind {
entry:
  %tmp_c = alloca i8, align 1
  %tmp_s = alloca i16, align 2
  %tmp_i = alloca i32, align 4
  %tmp_Ui = alloca i32, align 4
  %tmp_LLi = alloca i64, align 8
  %tmp_f = alloca float, align 4
  %tmp_d = alloca double, align 8
  %tmp_vp = alloca i8*, align 8
  %tmp_vCp = alloca i8*, align 8
  %tmp_cp = alloca i8*, align 8
  %tmp_cCp = alloca i8*, align 8
  %tmp_ip = alloca i32*, align 8
  %tmp_fp = alloca float*, align 8
  %tmp_fCp = alloca float*, align 8
  %tmp_dp = alloca double*, align 8
  %tmp_dCp = alloca double*, align 8
  %tmp_V2ip = alloca <2 x i32>*, align 8
  %tmp_V1LLip = alloca <1 x i64>*, align 8
  %tmp_V2LLip = alloca <2 x i64>*, align 8
  %tmp_V8c = alloca <8 x i8>, align 8
  %tmp_V4s = alloca <4 x i16>, align 8
  %tmp_V2i = alloca <2 x i32>, align 8
  %tmp_V1LLi = alloca <1 x i64>, align 8
  %tmp_V16c = alloca <16 x i8>, align 16
  %tmp_V8s = alloca <8 x i16>, align 16
  %tmp_V4i = alloca <4 x i32>, align 16
  %tmp_V2LLi = alloca <2 x i64>, align 16
  %tmp_V4f = alloca <4 x float>, align 16
  %tmp_V2d = alloca <2 x double>, align 16
  %tmp_V2dp = alloca <2 x double>*, align 8
  %tmp_V4fp = alloca <4 x float>*, align 8
  %tmp_V2dCp = alloca <2 x double>*, align 8
  %tmp_V4fCp = alloca <4 x float>*, align 8
  %tmp_V32c = alloca <32 x i8>, align 32
  %tmp_V4d = alloca <4 x double>, align 32
  %tmp_V8f = alloca <8 x float>, align 32
  %tmp_V4LLi = alloca <4 x i64>, align 32
  %tmp_V8i = alloca <8 x i32>, align 32
  %tmp_V4LLip = alloca <4 x i64>*, align 8
  %tmp_V4dp = alloca <4 x double>*, align 8
  %tmp_V8fp = alloca <8 x float>*, align 8
  %tmp_V4dCp = alloca <4 x double>*, align 8
  %tmp_V8fCp = alloca <8 x float>*, align 8
  %0 = load <4 x float>* %tmp_V4f, align 16
  %1 = load <4 x float>* %tmp_V4f, align 16
  %2 = call i32 @llvm.x86.sse.comieq.ss(<4 x float> %0, <4 x float> %1)
  store i32 %2, i32* %tmp_i, align 4
  %3 = load <4 x float>* %tmp_V4f, align 16
  %4 = load <4 x float>* %tmp_V4f, align 16
  %5 = call i32 @llvm.x86.sse.comilt.ss(<4 x float> %3, <4 x float> %4)
  store i32 %5, i32* %tmp_i, align 4
  %6 = load <4 x float>* %tmp_V4f, align 16
  %7 = load <4 x float>* %tmp_V4f, align 16
  %8 = call i32 @llvm.x86.sse.comile.ss(<4 x float> %6, <4 x float> %7)
  store i32 %8, i32* %tmp_i, align 4
  %9 = load <4 x float>* %tmp_V4f, align 16
  %10 = load <4 x float>* %tmp_V4f, align 16
  %11 = call i32 @llvm.x86.sse.comigt.ss(<4 x float> %9, <4 x float> %10)
  store i32 %11, i32* %tmp_i, align 4
  %12 = load <4 x float>* %tmp_V4f, align 16
  %13 = load <4 x float>* %tmp_V4f, align 16
  %14 = call i32 @llvm.x86.sse.comige.ss(<4 x float> %12, <4 x float> %13)
  store i32 %14, i32* %tmp_i, align 4
  %15 = load <4 x float>* %tmp_V4f, align 16
  %16 = load <4 x float>* %tmp_V4f, align 16
  %17 = call i32 @llvm.x86.sse.comineq.ss(<4 x float> %15, <4 x float> %16)
  store i32 %17, i32* %tmp_i, align 4
  %18 = load <4 x float>* %tmp_V4f, align 16
  %19 = load <4 x float>* %tmp_V4f, align 16
  %20 = call i32 @llvm.x86.sse.ucomieq.ss(<4 x float> %18, <4 x float> %19)
  store i32 %20, i32* %tmp_i, align 4
  %21 = load <4 x float>* %tmp_V4f, align 16
  %22 = load <4 x float>* %tmp_V4f, align 16
  %23 = call i32 @llvm.x86.sse.ucomilt.ss(<4 x float> %21, <4 x float> %22)
  store i32 %23, i32* %tmp_i, align 4
  %24 = load <4 x float>* %tmp_V4f, align 16
  %25 = load <4 x float>* %tmp_V4f, align 16
  %26 = call i32 @llvm.x86.sse.ucomile.ss(<4 x float> %24, <4 x float> %25)
  store i32 %26, i32* %tmp_i, align 4
  %27 = load <4 x float>* %tmp_V4f, align 16
  %28 = load <4 x float>* %tmp_V4f, align 16
  %29 = call i32 @llvm.x86.sse.ucomigt.ss(<4 x float> %27, <4 x float> %28)
  store i32 %29, i32* %tmp_i, align 4
  %30 = load <4 x float>* %tmp_V4f, align 16
  %31 = load <4 x float>* %tmp_V4f, align 16
  %32 = call i32 @llvm.x86.sse.ucomige.ss(<4 x float> %30, <4 x float> %31)
  store i32 %32, i32* %tmp_i, align 4
  %33 = load <4 x float>* %tmp_V4f, align 16
  %34 = load <4 x float>* %tmp_V4f, align 16
  %35 = call i32 @llvm.x86.sse.ucomineq.ss(<4 x float> %33, <4 x float> %34)
  store i32 %35, i32* %tmp_i, align 4
  %36 = load <2 x double>* %tmp_V2d, align 16
  %37 = load <2 x double>* %tmp_V2d, align 16
  %38 = call i32 @llvm.x86.sse2.comieq.sd(<2 x double> %36, <2 x double> %37)
  store i32 %38, i32* %tmp_i, align 4
  %39 = load <2 x double>* %tmp_V2d, align 16
  %40 = load <2 x double>* %tmp_V2d, align 16
  %41 = call i32 @llvm.x86.sse2.comilt.sd(<2 x double> %39, <2 x double> %40)
  store i32 %41, i32* %tmp_i, align 4
  %42 = load <2 x double>* %tmp_V2d, align 16
  %43 = load <2 x double>* %tmp_V2d, align 16
  %44 = call i32 @llvm.x86.sse2.comile.sd(<2 x double> %42, <2 x double> %43)
  store i32 %44, i32* %tmp_i, align 4
  %45 = load <2 x double>* %tmp_V2d, align 16
  %46 = load <2 x double>* %tmp_V2d, align 16
  %47 = call i32 @llvm.x86.sse2.comigt.sd(<2 x double> %45, <2 x double> %46)
  store i32 %47, i32* %tmp_i, align 4
  %48 = load <2 x double>* %tmp_V2d, align 16
  %49 = load <2 x double>* %tmp_V2d, align 16
  %50 = call i32 @llvm.x86.sse2.comige.sd(<2 x double> %48, <2 x double> %49)
  store i32 %50, i32* %tmp_i, align 4
  %51 = load <2 x double>* %tmp_V2d, align 16
  %52 = load <2 x double>* %tmp_V2d, align 16
  %53 = call i32 @llvm.x86.sse2.comineq.sd(<2 x double> %51, <2 x double> %52)
  store i32 %53, i32* %tmp_i, align 4
  %54 = load <2 x double>* %tmp_V2d, align 16
  %55 = load <2 x double>* %tmp_V2d, align 16
  %56 = call i32 @llvm.x86.sse2.ucomieq.sd(<2 x double> %54, <2 x double> %55)
  store i32 %56, i32* %tmp_i, align 4
  %57 = load <2 x double>* %tmp_V2d, align 16
  %58 = load <2 x double>* %tmp_V2d, align 16
  %59 = call i32 @llvm.x86.sse2.ucomilt.sd(<2 x double> %57, <2 x double> %58)
  store i32 %59, i32* %tmp_i, align 4
  %60 = load <2 x double>* %tmp_V2d, align 16
  %61 = load <2 x double>* %tmp_V2d, align 16
  %62 = call i32 @llvm.x86.sse2.ucomile.sd(<2 x double> %60, <2 x double> %61)
  store i32 %62, i32* %tmp_i, align 4
  %63 = load <2 x double>* %tmp_V2d, align 16
  %64 = load <2 x double>* %tmp_V2d, align 16
  %65 = call i32 @llvm.x86.sse2.ucomigt.sd(<2 x double> %63, <2 x double> %64)
  store i32 %65, i32* %tmp_i, align 4
  %66 = load <2 x double>* %tmp_V2d, align 16
  %67 = load <2 x double>* %tmp_V2d, align 16
  %68 = call i32 @llvm.x86.sse2.ucomige.sd(<2 x double> %66, <2 x double> %67)
  store i32 %68, i32* %tmp_i, align 4
  %69 = load <2 x double>* %tmp_V2d, align 16
  %70 = load <2 x double>* %tmp_V2d, align 16
  %71 = call i32 @llvm.x86.sse2.ucomineq.sd(<2 x double> %69, <2 x double> %70)
  store i32 %71, i32* %tmp_i, align 4
  %72 = load <4 x float>* %tmp_V4f, align 16
  %73 = load <4 x float>* %tmp_V4f, align 16
  %74 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %72, <4 x float> %73, i8 0)
  store <4 x float> %74, <4 x float>* %tmp_V4f, align 16
  %75 = load <4 x float>* %tmp_V4f, align 16
  %76 = load <4 x float>* %tmp_V4f, align 16
  %77 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %75, <4 x float> %76, i8 1)
  store <4 x float> %77, <4 x float>* %tmp_V4f, align 16
  %78 = load <4 x float>* %tmp_V4f, align 16
  %79 = load <4 x float>* %tmp_V4f, align 16
  %80 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %78, <4 x float> %79, i8 2)
  store <4 x float> %80, <4 x float>* %tmp_V4f, align 16
  %81 = load <4 x float>* %tmp_V4f, align 16
  %82 = load <4 x float>* %tmp_V4f, align 16
  %83 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %81, <4 x float> %82, i8 3)
  store <4 x float> %83, <4 x float>* %tmp_V4f, align 16
  %84 = load <4 x float>* %tmp_V4f, align 16
  %85 = load <4 x float>* %tmp_V4f, align 16
  %86 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %84, <4 x float> %85, i8 4)
  store <4 x float> %86, <4 x float>* %tmp_V4f, align 16
  %87 = load <4 x float>* %tmp_V4f, align 16
  %88 = load <4 x float>* %tmp_V4f, align 16
  %89 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %87, <4 x float> %88, i8 5)
  store <4 x float> %89, <4 x float>* %tmp_V4f, align 16
  %90 = load <4 x float>* %tmp_V4f, align 16
  %91 = load <4 x float>* %tmp_V4f, align 16
  %92 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %90, <4 x float> %91, i8 6)
  store <4 x float> %92, <4 x float>* %tmp_V4f, align 16
  %93 = load <4 x float>* %tmp_V4f, align 16
  %94 = load <4 x float>* %tmp_V4f, align 16
  %95 = call <4 x float> @llvm.x86.sse.cmp.ps(<4 x float> %93, <4 x float> %94, i8 7)
  store <4 x float> %95, <4 x float>* %tmp_V4f, align 16
  %96 = load <4 x float>* %tmp_V4f, align 16
  %97 = load <4 x float>* %tmp_V4f, align 16
  %98 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %96, <4 x float> %97, i8 0)
  store <4 x float> %98, <4 x float>* %tmp_V4f, align 16
  %99 = load <4 x float>* %tmp_V4f, align 16
  %100 = load <4 x float>* %tmp_V4f, align 16
  %101 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %99, <4 x float> %100, i8 1)
  store <4 x float> %101, <4 x float>* %tmp_V4f, align 16
  %102 = load <4 x float>* %tmp_V4f, align 16
  %103 = load <4 x float>* %tmp_V4f, align 16
  %104 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %102, <4 x float> %103, i8 2)
  store <4 x float> %104, <4 x float>* %tmp_V4f, align 16
  %105 = load <4 x float>* %tmp_V4f, align 16
  %106 = load <4 x float>* %tmp_V4f, align 16
  %107 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %105, <4 x float> %106, i8 3)
  store <4 x float> %107, <4 x float>* %tmp_V4f, align 16
  %108 = load <4 x float>* %tmp_V4f, align 16
  %109 = load <4 x float>* %tmp_V4f, align 16
  %110 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %108, <4 x float> %109, i8 4)
  store <4 x float> %110, <4 x float>* %tmp_V4f, align 16
  %111 = load <4 x float>* %tmp_V4f, align 16
  %112 = load <4 x float>* %tmp_V4f, align 16
  %113 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %111, <4 x float> %112, i8 5)
  store <4 x float> %113, <4 x float>* %tmp_V4f, align 16
  %114 = load <4 x float>* %tmp_V4f, align 16
  %115 = load <4 x float>* %tmp_V4f, align 16
  %116 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %114, <4 x float> %115, i8 6)
  store <4 x float> %116, <4 x float>* %tmp_V4f, align 16
  %117 = load <4 x float>* %tmp_V4f, align 16
  %118 = load <4 x float>* %tmp_V4f, align 16
  %119 = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> %117, <4 x float> %118, i8 7)
  store <4 x float> %119, <4 x float>* %tmp_V4f, align 16
  %120 = load <4 x float>* %tmp_V4f, align 16
  %121 = load <4 x float>* %tmp_V4f, align 16
  %122 = call <4 x float> @llvm.x86.sse.min.ps(<4 x float> %120, <4 x float> %121)
  store <4 x float> %122, <4 x float>* %tmp_V4f, align 16
  %123 = load <4 x float>* %tmp_V4f, align 16
  %124 = load <4 x float>* %tmp_V4f, align 16
  %125 = call <4 x float> @llvm.x86.sse.max.ps(<4 x float> %123, <4 x float> %124)
  store <4 x float> %125, <4 x float>* %tmp_V4f, align 16
  %126 = load <4 x float>* %tmp_V4f, align 16
  %127 = load <4 x float>* %tmp_V4f, align 16
  %128 = call <4 x float> @llvm.x86.sse.min.ss(<4 x float> %126, <4 x float> %127)
  store <4 x float> %128, <4 x float>* %tmp_V4f, align 16
  %129 = load <4 x float>* %tmp_V4f, align 16
  %130 = load <4 x float>* %tmp_V4f, align 16
  %131 = call <4 x float> @llvm.x86.sse.max.ss(<4 x float> %129, <4 x float> %130)
  store <4 x float> %131, <4 x float>* %tmp_V4f, align 16
  %132 = load <8 x i8>* %tmp_V8c, align 8
  %133 = bitcast <8 x i8> %132 to x86_mmx
  %134 = load <8 x i8>* %tmp_V8c, align 8
  %135 = bitcast <8 x i8> %134 to x86_mmx
  %136 = call x86_mmx @llvm.x86.mmx.padds.b(x86_mmx %133, x86_mmx %135)
  %137 = bitcast x86_mmx %136 to <8 x i8>
  store <8 x i8> %137, <8 x i8>* %tmp_V8c, align 8
  %138 = load <4 x i16>* %tmp_V4s, align 8
  %139 = bitcast <4 x i16> %138 to x86_mmx
  %140 = load <4 x i16>* %tmp_V4s, align 8
  %141 = bitcast <4 x i16> %140 to x86_mmx
  %142 = call x86_mmx @llvm.x86.mmx.padds.w(x86_mmx %139, x86_mmx %141)
  %143 = bitcast x86_mmx %142 to <4 x i16>
  store <4 x i16> %143, <4 x i16>* %tmp_V4s, align 8
  %144 = load <8 x i8>* %tmp_V8c, align 8
  %145 = bitcast <8 x i8> %144 to x86_mmx
  %146 = load <8 x i8>* %tmp_V8c, align 8
  %147 = bitcast <8 x i8> %146 to x86_mmx
  %148 = call x86_mmx @llvm.x86.mmx.psubs.b(x86_mmx %145, x86_mmx %147)
  %149 = bitcast x86_mmx %148 to <8 x i8>
  store <8 x i8> %149, <8 x i8>* %tmp_V8c, align 8
  %150 = load <4 x i16>* %tmp_V4s, align 8
  %151 = bitcast <4 x i16> %150 to x86_mmx
  %152 = load <4 x i16>* %tmp_V4s, align 8
  %153 = bitcast <4 x i16> %152 to x86_mmx
  %154 = call x86_mmx @llvm.x86.mmx.psubs.w(x86_mmx %151, x86_mmx %153)
  %155 = bitcast x86_mmx %154 to <4 x i16>
  store <4 x i16> %155, <4 x i16>* %tmp_V4s, align 8
  %156 = load <8 x i8>* %tmp_V8c, align 8
  %157 = bitcast <8 x i8> %156 to x86_mmx
  %158 = load <8 x i8>* %tmp_V8c, align 8
  %159 = bitcast <8 x i8> %158 to x86_mmx
  %160 = call x86_mmx @llvm.x86.mmx.paddus.b(x86_mmx %157, x86_mmx %159)
  %161 = bitcast x86_mmx %160 to <8 x i8>
  store <8 x i8> %161, <8 x i8>* %tmp_V8c, align 8
  %162 = load <4 x i16>* %tmp_V4s, align 8
  %163 = bitcast <4 x i16> %162 to x86_mmx
  %164 = load <4 x i16>* %tmp_V4s, align 8
  %165 = bitcast <4 x i16> %164 to x86_mmx
  %166 = call x86_mmx @llvm.x86.mmx.paddus.w(x86_mmx %163, x86_mmx %165)
  %167 = bitcast x86_mmx %166 to <4 x i16>
  store <4 x i16> %167, <4 x i16>* %tmp_V4s, align 8
  %168 = load <8 x i8>* %tmp_V8c, align 8
  %169 = bitcast <8 x i8> %168 to x86_mmx
  %170 = load <8 x i8>* %tmp_V8c, align 8
  %171 = bitcast <8 x i8> %170 to x86_mmx
  %172 = call x86_mmx @llvm.x86.mmx.psubus.b(x86_mmx %169, x86_mmx %171)
  %173 = bitcast x86_mmx %172 to <8 x i8>
  store <8 x i8> %173, <8 x i8>* %tmp_V8c, align 8
  %174 = load <4 x i16>* %tmp_V4s, align 8
  %175 = bitcast <4 x i16> %174 to x86_mmx
  %176 = load <4 x i16>* %tmp_V4s, align 8
  %177 = bitcast <4 x i16> %176 to x86_mmx
  %178 = call x86_mmx @llvm.x86.mmx.psubus.w(x86_mmx %175, x86_mmx %177)
  %179 = bitcast x86_mmx %178 to <4 x i16>
  store <4 x i16> %179, <4 x i16>* %tmp_V4s, align 8
  %180 = load <4 x i16>* %tmp_V4s, align 8
  %181 = bitcast <4 x i16> %180 to x86_mmx
  %182 = load <4 x i16>* %tmp_V4s, align 8
  %183 = bitcast <4 x i16> %182 to x86_mmx
  %184 = call x86_mmx @llvm.x86.mmx.pmulh.w(x86_mmx %181, x86_mmx %183)
  %185 = bitcast x86_mmx %184 to <4 x i16>
  store <4 x i16> %185, <4 x i16>* %tmp_V4s, align 8
  %186 = load <4 x i16>* %tmp_V4s, align 8
  %187 = bitcast <4 x i16> %186 to x86_mmx
  %188 = load <4 x i16>* %tmp_V4s, align 8
  %189 = bitcast <4 x i16> %188 to x86_mmx
  %190 = call x86_mmx @llvm.x86.mmx.pmulhu.w(x86_mmx %187, x86_mmx %189)
  %191 = bitcast x86_mmx %190 to <4 x i16>
  store <4 x i16> %191, <4 x i16>* %tmp_V4s, align 8
  %192 = load <8 x i8>* %tmp_V8c, align 8
  %193 = bitcast <8 x i8> %192 to x86_mmx
  %194 = load <8 x i8>* %tmp_V8c, align 8
  %195 = bitcast <8 x i8> %194 to x86_mmx
  %196 = call x86_mmx @llvm.x86.mmx.pavg.b(x86_mmx %193, x86_mmx %195)
  %197 = bitcast x86_mmx %196 to <8 x i8>
  store <8 x i8> %197, <8 x i8>* %tmp_V8c, align 8
  %198 = load <4 x i16>* %tmp_V4s, align 8
  %199 = bitcast <4 x i16> %198 to x86_mmx
  %200 = load <4 x i16>* %tmp_V4s, align 8
  %201 = bitcast <4 x i16> %200 to x86_mmx
  %202 = call x86_mmx @llvm.x86.mmx.pavg.w(x86_mmx %199, x86_mmx %201)
  %203 = bitcast x86_mmx %202 to <4 x i16>
  store <4 x i16> %203, <4 x i16>* %tmp_V4s, align 8
  %204 = load <8 x i8>* %tmp_V8c, align 8
  %205 = bitcast <8 x i8> %204 to x86_mmx
  %206 = load <8 x i8>* %tmp_V8c, align 8
  %207 = bitcast <8 x i8> %206 to x86_mmx
  %208 = call x86_mmx @llvm.x86.mmx.pcmpeq.b(x86_mmx %205, x86_mmx %207)
  %209 = bitcast x86_mmx %208 to <8 x i8>
  store <8 x i8> %209, <8 x i8>* %tmp_V8c, align 8
  %210 = load <4 x i16>* %tmp_V4s, align 8
  %211 = bitcast <4 x i16> %210 to x86_mmx
  %212 = load <4 x i16>* %tmp_V4s, align 8
  %213 = bitcast <4 x i16> %212 to x86_mmx
  %214 = call x86_mmx @llvm.x86.mmx.pcmpeq.w(x86_mmx %211, x86_mmx %213)
  %215 = bitcast x86_mmx %214 to <4 x i16>
  store <4 x i16> %215, <4 x i16>* %tmp_V4s, align 8
  %216 = load <2 x i32>* %tmp_V2i, align 8
  %217 = bitcast <2 x i32> %216 to x86_mmx
  %218 = load <2 x i32>* %tmp_V2i, align 8
  %219 = bitcast <2 x i32> %218 to x86_mmx
  %220 = call x86_mmx @llvm.x86.mmx.pcmpeq.d(x86_mmx %217, x86_mmx %219)
  %221 = bitcast x86_mmx %220 to <2 x i32>
  store <2 x i32> %221, <2 x i32>* %tmp_V2i, align 8
  %222 = load <8 x i8>* %tmp_V8c, align 8
  %223 = bitcast <8 x i8> %222 to x86_mmx
  %224 = load <8 x i8>* %tmp_V8c, align 8
  %225 = bitcast <8 x i8> %224 to x86_mmx
  %226 = call x86_mmx @llvm.x86.mmx.pcmpgt.b(x86_mmx %223, x86_mmx %225)
  %227 = bitcast x86_mmx %226 to <8 x i8>
  store <8 x i8> %227, <8 x i8>* %tmp_V8c, align 8
  %228 = load <4 x i16>* %tmp_V4s, align 8
  %229 = bitcast <4 x i16> %228 to x86_mmx
  %230 = load <4 x i16>* %tmp_V4s, align 8
  %231 = bitcast <4 x i16> %230 to x86_mmx
  %232 = call x86_mmx @llvm.x86.mmx.pcmpgt.w(x86_mmx %229, x86_mmx %231)
  %233 = bitcast x86_mmx %232 to <4 x i16>
  store <4 x i16> %233, <4 x i16>* %tmp_V4s, align 8
  %234 = load <2 x i32>* %tmp_V2i, align 8
  %235 = bitcast <2 x i32> %234 to x86_mmx
  %236 = load <2 x i32>* %tmp_V2i, align 8
  %237 = bitcast <2 x i32> %236 to x86_mmx
  %238 = call x86_mmx @llvm.x86.mmx.pcmpgt.d(x86_mmx %235, x86_mmx %237)
  %239 = bitcast x86_mmx %238 to <2 x i32>
  store <2 x i32> %239, <2 x i32>* %tmp_V2i, align 8
  %240 = load <8 x i8>* %tmp_V8c, align 8
  %241 = bitcast <8 x i8> %240 to x86_mmx
  %242 = load <8 x i8>* %tmp_V8c, align 8
  %243 = bitcast <8 x i8> %242 to x86_mmx
  %244 = call x86_mmx @llvm.x86.mmx.pmaxu.b(x86_mmx %241, x86_mmx %243)
  %245 = bitcast x86_mmx %244 to <8 x i8>
  store <8 x i8> %245, <8 x i8>* %tmp_V8c, align 8
  %246 = load <4 x i16>* %tmp_V4s, align 8
  %247 = bitcast <4 x i16> %246 to x86_mmx
  %248 = load <4 x i16>* %tmp_V4s, align 8
  %249 = bitcast <4 x i16> %248 to x86_mmx
  %250 = call x86_mmx @llvm.x86.mmx.pmaxs.w(x86_mmx %247, x86_mmx %249)
  %251 = bitcast x86_mmx %250 to <4 x i16>
  store <4 x i16> %251, <4 x i16>* %tmp_V4s, align 8
  %252 = load <8 x i8>* %tmp_V8c, align 8
  %253 = bitcast <8 x i8> %252 to x86_mmx
  %254 = load <8 x i8>* %tmp_V8c, align 8
  %255 = bitcast <8 x i8> %254 to x86_mmx
  %256 = call x86_mmx @llvm.x86.mmx.pminu.b(x86_mmx %253, x86_mmx %255)
  %257 = bitcast x86_mmx %256 to <8 x i8>
  store <8 x i8> %257, <8 x i8>* %tmp_V8c, align 8
  %258 = load <4 x i16>* %tmp_V4s, align 8
  %259 = bitcast <4 x i16> %258 to x86_mmx
  %260 = load <4 x i16>* %tmp_V4s, align 8
  %261 = bitcast <4 x i16> %260 to x86_mmx
  %262 = call x86_mmx @llvm.x86.mmx.pmins.w(x86_mmx %259, x86_mmx %261)
  %263 = bitcast x86_mmx %262 to <4 x i16>
  store <4 x i16> %263, <4 x i16>* %tmp_V4s, align 8
  %264 = load <2 x double>* %tmp_V2d, align 16
  %265 = load <2 x double>* %tmp_V2d, align 16
  %266 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %264, <2 x double> %265, i8 0)
  store <2 x double> %266, <2 x double>* %tmp_V2d, align 16
  %267 = load <2 x double>* %tmp_V2d, align 16
  %268 = load <2 x double>* %tmp_V2d, align 16
  %269 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %267, <2 x double> %268, i8 1)
  store <2 x double> %269, <2 x double>* %tmp_V2d, align 16
  %270 = load <2 x double>* %tmp_V2d, align 16
  %271 = load <2 x double>* %tmp_V2d, align 16
  %272 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %270, <2 x double> %271, i8 2)
  store <2 x double> %272, <2 x double>* %tmp_V2d, align 16
  %273 = load <2 x double>* %tmp_V2d, align 16
  %274 = load <2 x double>* %tmp_V2d, align 16
  %275 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %273, <2 x double> %274, i8 3)
  store <2 x double> %275, <2 x double>* %tmp_V2d, align 16
  %276 = load <2 x double>* %tmp_V2d, align 16
  %277 = load <2 x double>* %tmp_V2d, align 16
  %278 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %276, <2 x double> %277, i8 4)
  store <2 x double> %278, <2 x double>* %tmp_V2d, align 16
  %279 = load <2 x double>* %tmp_V2d, align 16
  %280 = load <2 x double>* %tmp_V2d, align 16
  %281 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %279, <2 x double> %280, i8 5)
  store <2 x double> %281, <2 x double>* %tmp_V2d, align 16
  %282 = load <2 x double>* %tmp_V2d, align 16
  %283 = load <2 x double>* %tmp_V2d, align 16
  %284 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %282, <2 x double> %283, i8 6)
  store <2 x double> %284, <2 x double>* %tmp_V2d, align 16
  %285 = load <2 x double>* %tmp_V2d, align 16
  %286 = load <2 x double>* %tmp_V2d, align 16
  %287 = call <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double> %285, <2 x double> %286, i8 7)
  store <2 x double> %287, <2 x double>* %tmp_V2d, align 16
  %288 = load <2 x double>* %tmp_V2d, align 16
  %289 = load <2 x double>* %tmp_V2d, align 16
  %290 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %288, <2 x double> %289, i8 0)
  store <2 x double> %290, <2 x double>* %tmp_V2d, align 16
  %291 = load <2 x double>* %tmp_V2d, align 16
  %292 = load <2 x double>* %tmp_V2d, align 16
  %293 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %291, <2 x double> %292, i8 1)
  store <2 x double> %293, <2 x double>* %tmp_V2d, align 16
  %294 = load <2 x double>* %tmp_V2d, align 16
  %295 = load <2 x double>* %tmp_V2d, align 16
  %296 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %294, <2 x double> %295, i8 2)
  store <2 x double> %296, <2 x double>* %tmp_V2d, align 16
  %297 = load <2 x double>* %tmp_V2d, align 16
  %298 = load <2 x double>* %tmp_V2d, align 16
  %299 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %297, <2 x double> %298, i8 3)
  store <2 x double> %299, <2 x double>* %tmp_V2d, align 16
  %300 = load <2 x double>* %tmp_V2d, align 16
  %301 = load <2 x double>* %tmp_V2d, align 16
  %302 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %300, <2 x double> %301, i8 4)
  store <2 x double> %302, <2 x double>* %tmp_V2d, align 16
  %303 = load <2 x double>* %tmp_V2d, align 16
  %304 = load <2 x double>* %tmp_V2d, align 16
  %305 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %303, <2 x double> %304, i8 5)
  store <2 x double> %305, <2 x double>* %tmp_V2d, align 16
  %306 = load <2 x double>* %tmp_V2d, align 16
  %307 = load <2 x double>* %tmp_V2d, align 16
  %308 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %306, <2 x double> %307, i8 6)
  store <2 x double> %308, <2 x double>* %tmp_V2d, align 16
  %309 = load <2 x double>* %tmp_V2d, align 16
  %310 = load <2 x double>* %tmp_V2d, align 16
  %311 = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> %309, <2 x double> %310, i8 7)
  store <2 x double> %311, <2 x double>* %tmp_V2d, align 16
  %312 = load <2 x double>* %tmp_V2d, align 16
  %313 = load <2 x double>* %tmp_V2d, align 16
  %314 = call <2 x double> @llvm.x86.sse2.min.pd(<2 x double> %312, <2 x double> %313)
  store <2 x double> %314, <2 x double>* %tmp_V2d, align 16
  %315 = load <2 x double>* %tmp_V2d, align 16
  %316 = load <2 x double>* %tmp_V2d, align 16
  %317 = call <2 x double> @llvm.x86.sse2.max.pd(<2 x double> %315, <2 x double> %316)
  store <2 x double> %317, <2 x double>* %tmp_V2d, align 16
  %318 = load <2 x double>* %tmp_V2d, align 16
  %319 = load <2 x double>* %tmp_V2d, align 16
  %320 = call <2 x double> @llvm.x86.sse2.min.sd(<2 x double> %318, <2 x double> %319)
  store <2 x double> %320, <2 x double>* %tmp_V2d, align 16
  %321 = load <2 x double>* %tmp_V2d, align 16
  %322 = load <2 x double>* %tmp_V2d, align 16
  %323 = call <2 x double> @llvm.x86.sse2.max.sd(<2 x double> %321, <2 x double> %322)
  store <2 x double> %323, <2 x double>* %tmp_V2d, align 16
  %324 = load <16 x i8>* %tmp_V16c, align 16
  %325 = load <16 x i8>* %tmp_V16c, align 16
  %326 = call <16 x i8> @llvm.x86.sse2.padds.b(<16 x i8> %324, <16 x i8> %325)
  store <16 x i8> %326, <16 x i8>* %tmp_V16c, align 16
  %327 = load <8 x i16>* %tmp_V8s, align 16
  %328 = load <8 x i16>* %tmp_V8s, align 16
  %329 = call <8 x i16> @llvm.x86.sse2.padds.w(<8 x i16> %327, <8 x i16> %328)
  store <8 x i16> %329, <8 x i16>* %tmp_V8s, align 16
  %330 = load <16 x i8>* %tmp_V16c, align 16
  %331 = load <16 x i8>* %tmp_V16c, align 16
  %332 = call <16 x i8> @llvm.x86.sse2.psubs.b(<16 x i8> %330, <16 x i8> %331)
  store <16 x i8> %332, <16 x i8>* %tmp_V16c, align 16
  %333 = load <8 x i16>* %tmp_V8s, align 16
  %334 = load <8 x i16>* %tmp_V8s, align 16
  %335 = call <8 x i16> @llvm.x86.sse2.psubs.w(<8 x i16> %333, <8 x i16> %334)
  store <8 x i16> %335, <8 x i16>* %tmp_V8s, align 16
  %336 = load <16 x i8>* %tmp_V16c, align 16
  %337 = load <16 x i8>* %tmp_V16c, align 16
  %338 = call <16 x i8> @llvm.x86.sse2.paddus.b(<16 x i8> %336, <16 x i8> %337)
  store <16 x i8> %338, <16 x i8>* %tmp_V16c, align 16
  %339 = load <8 x i16>* %tmp_V8s, align 16
  %340 = load <8 x i16>* %tmp_V8s, align 16
  %341 = call <8 x i16> @llvm.x86.sse2.paddus.w(<8 x i16> %339, <8 x i16> %340)
  store <8 x i16> %341, <8 x i16>* %tmp_V8s, align 16
  %342 = load <16 x i8>* %tmp_V16c, align 16
  %343 = load <16 x i8>* %tmp_V16c, align 16
  %344 = call <16 x i8> @llvm.x86.sse2.psubus.b(<16 x i8> %342, <16 x i8> %343)
  store <16 x i8> %344, <16 x i8>* %tmp_V16c, align 16
  %345 = load <8 x i16>* %tmp_V8s, align 16
  %346 = load <8 x i16>* %tmp_V8s, align 16
  %347 = call <8 x i16> @llvm.x86.sse2.psubus.w(<8 x i16> %345, <8 x i16> %346)
  store <8 x i16> %347, <8 x i16>* %tmp_V8s, align 16
  %348 = load <8 x i16>* %tmp_V8s, align 16
  %349 = load <8 x i16>* %tmp_V8s, align 16
  %350 = call <8 x i16> @llvm.x86.sse2.pmulh.w(<8 x i16> %348, <8 x i16> %349)
  store <8 x i16> %350, <8 x i16>* %tmp_V8s, align 16
  %351 = load <16 x i8>* %tmp_V16c, align 16
  %352 = load <16 x i8>* %tmp_V16c, align 16
  %353 = call <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8> %351, <16 x i8> %352)
  store <16 x i8> %353, <16 x i8>* %tmp_V16c, align 16
  %354 = load <8 x i16>* %tmp_V8s, align 16
  %355 = load <8 x i16>* %tmp_V8s, align 16
  %356 = call <8 x i16> @llvm.x86.sse2.pavg.w(<8 x i16> %354, <8 x i16> %355)
  store <8 x i16> %356, <8 x i16>* %tmp_V8s, align 16
  %357 = load <16 x i8>* %tmp_V16c, align 16
  %358 = load <16 x i8>* %tmp_V16c, align 16
  %359 = call <16 x i8> @llvm.x86.sse2.pmaxu.b(<16 x i8> %357, <16 x i8> %358)
  store <16 x i8> %359, <16 x i8>* %tmp_V16c, align 16
  %360 = load <8 x i16>* %tmp_V8s, align 16
  %361 = load <8 x i16>* %tmp_V8s, align 16
  %362 = call <8 x i16> @llvm.x86.sse2.pmaxs.w(<8 x i16> %360, <8 x i16> %361)
  store <8 x i16> %362, <8 x i16>* %tmp_V8s, align 16
  %363 = load <16 x i8>* %tmp_V16c, align 16
  %364 = load <16 x i8>* %tmp_V16c, align 16
  %365 = call <16 x i8> @llvm.x86.sse2.pminu.b(<16 x i8> %363, <16 x i8> %364)
  store <16 x i8> %365, <16 x i8>* %tmp_V16c, align 16
  %366 = load <8 x i16>* %tmp_V8s, align 16
  %367 = load <8 x i16>* %tmp_V8s, align 16
  %368 = call <8 x i16> @llvm.x86.sse2.pmins.w(<8 x i16> %366, <8 x i16> %367)
  store <8 x i16> %368, <8 x i16>* %tmp_V8s, align 16
  %369 = load <8 x i16>* %tmp_V8s, align 16
  %370 = load <8 x i16>* %tmp_V8s, align 16
  %371 = call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> %369, <8 x i16> %370)
  %372 = bitcast <16 x i8> %371 to <8 x i16>
  store <8 x i16> %372, <8 x i16>* %tmp_V8s, align 16
  %373 = load <4 x i32>* %tmp_V4i, align 16
  %374 = load <4 x i32>* %tmp_V4i, align 16
  %375 = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> %373, <4 x i32> %374)
  %376 = bitcast <8 x i16> %375 to <4 x i32>
  store <4 x i32> %376, <4 x i32>* %tmp_V4i, align 16
  %377 = load <8 x i16>* %tmp_V8s, align 16
  %378 = load <8 x i16>* %tmp_V8s, align 16
  %379 = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> %377, <8 x i16> %378)
  %380 = bitcast <16 x i8> %379 to <8 x i16>
  store <8 x i16> %380, <8 x i16>* %tmp_V8s, align 16
  %381 = load <8 x i16>* %tmp_V8s, align 16
  %382 = load <8 x i16>* %tmp_V8s, align 16
  %383 = call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> %381, <8 x i16> %382)
  store <8 x i16> %383, <8 x i16>* %tmp_V8s, align 16
  %384 = load <4 x float>* %tmp_V4f, align 16
  %385 = load <4 x float>* %tmp_V4f, align 16
  %386 = call <4 x float> @llvm.x86.sse3.addsub.ps(<4 x float> %384, <4 x float> %385)
  store <4 x float> %386, <4 x float>* %tmp_V4f, align 16
  %387 = load <2 x double>* %tmp_V2d, align 16
  %388 = load <2 x double>* %tmp_V2d, align 16
  %389 = call <2 x double> @llvm.x86.sse3.addsub.pd(<2 x double> %387, <2 x double> %388)
  store <2 x double> %389, <2 x double>* %tmp_V2d, align 16
  %390 = load <4 x float>* %tmp_V4f, align 16
  %391 = load <4 x float>* %tmp_V4f, align 16
  %392 = call <4 x float> @llvm.x86.sse3.hadd.ps(<4 x float> %390, <4 x float> %391)
  store <4 x float> %392, <4 x float>* %tmp_V4f, align 16
  %393 = load <2 x double>* %tmp_V2d, align 16
  %394 = load <2 x double>* %tmp_V2d, align 16
  %395 = call <2 x double> @llvm.x86.sse3.hadd.pd(<2 x double> %393, <2 x double> %394)
  store <2 x double> %395, <2 x double>* %tmp_V2d, align 16
  %396 = load <4 x float>* %tmp_V4f, align 16
  %397 = load <4 x float>* %tmp_V4f, align 16
  %398 = call <4 x float> @llvm.x86.sse3.hsub.ps(<4 x float> %396, <4 x float> %397)
  store <4 x float> %398, <4 x float>* %tmp_V4f, align 16
  %399 = load <2 x double>* %tmp_V2d, align 16
  %400 = load <2 x double>* %tmp_V2d, align 16
  %401 = call <2 x double> @llvm.x86.sse3.hsub.pd(<2 x double> %399, <2 x double> %400)
  store <2 x double> %401, <2 x double>* %tmp_V2d, align 16
  %402 = load <8 x i16>* %tmp_V8s, align 16
  %403 = load <8 x i16>* %tmp_V8s, align 16
  %404 = call <8 x i16> @llvm.x86.ssse3.phadd.w.128(<8 x i16> %402, <8 x i16> %403)
  store <8 x i16> %404, <8 x i16>* %tmp_V8s, align 16
  %405 = load <4 x i16>* %tmp_V4s, align 8
  %406 = bitcast <4 x i16> %405 to x86_mmx
  %407 = load <4 x i16>* %tmp_V4s, align 8
  %408 = bitcast <4 x i16> %407 to x86_mmx
  %409 = call x86_mmx @llvm.x86.ssse3.phadd.w(x86_mmx %406, x86_mmx %408)
  %410 = bitcast x86_mmx %409 to <4 x i16>
  store <4 x i16> %410, <4 x i16>* %tmp_V4s, align 8
  %411 = load <4 x i32>* %tmp_V4i, align 16
  %412 = load <4 x i32>* %tmp_V4i, align 16
  %413 = call <4 x i32> @llvm.x86.ssse3.phadd.d.128(<4 x i32> %411, <4 x i32> %412)
  store <4 x i32> %413, <4 x i32>* %tmp_V4i, align 16
  %414 = load <2 x i32>* %tmp_V2i, align 8
  %415 = bitcast <2 x i32> %414 to x86_mmx
  %416 = load <2 x i32>* %tmp_V2i, align 8
  %417 = bitcast <2 x i32> %416 to x86_mmx
  %418 = call x86_mmx @llvm.x86.ssse3.phadd.d(x86_mmx %415, x86_mmx %417)
  %419 = bitcast x86_mmx %418 to <2 x i32>
  store <2 x i32> %419, <2 x i32>* %tmp_V2i, align 8
  %420 = load <8 x i16>* %tmp_V8s, align 16
  %421 = load <8 x i16>* %tmp_V8s, align 16
  %422 = call <8 x i16> @llvm.x86.ssse3.phadd.sw.128(<8 x i16> %420, <8 x i16> %421)
  store <8 x i16> %422, <8 x i16>* %tmp_V8s, align 16
  %423 = load <4 x i16>* %tmp_V4s, align 8
  %424 = bitcast <4 x i16> %423 to x86_mmx
  %425 = load <4 x i16>* %tmp_V4s, align 8
  %426 = bitcast <4 x i16> %425 to x86_mmx
  %427 = call x86_mmx @llvm.x86.ssse3.phadd.sw(x86_mmx %424, x86_mmx %426)
  %428 = bitcast x86_mmx %427 to <4 x i16>
  store <4 x i16> %428, <4 x i16>* %tmp_V4s, align 8
  %429 = load <8 x i16>* %tmp_V8s, align 16
  %430 = load <8 x i16>* %tmp_V8s, align 16
  %431 = call <8 x i16> @llvm.x86.ssse3.phsub.w.128(<8 x i16> %429, <8 x i16> %430)
  store <8 x i16> %431, <8 x i16>* %tmp_V8s, align 16
  %432 = load <4 x i16>* %tmp_V4s, align 8
  %433 = bitcast <4 x i16> %432 to x86_mmx
  %434 = load <4 x i16>* %tmp_V4s, align 8
  %435 = bitcast <4 x i16> %434 to x86_mmx
  %436 = call x86_mmx @llvm.x86.ssse3.phsub.w(x86_mmx %433, x86_mmx %435)
  %437 = bitcast x86_mmx %436 to <4 x i16>
  store <4 x i16> %437, <4 x i16>* %tmp_V4s, align 8
  %438 = load <4 x i32>* %tmp_V4i, align 16
  %439 = load <4 x i32>* %tmp_V4i, align 16
  %440 = call <4 x i32> @llvm.x86.ssse3.phsub.d.128(<4 x i32> %438, <4 x i32> %439)
  store <4 x i32> %440, <4 x i32>* %tmp_V4i, align 16
  %441 = load <2 x i32>* %tmp_V2i, align 8
  %442 = bitcast <2 x i32> %441 to x86_mmx
  %443 = load <2 x i32>* %tmp_V2i, align 8
  %444 = bitcast <2 x i32> %443 to x86_mmx
  %445 = call x86_mmx @llvm.x86.ssse3.phsub.d(x86_mmx %442, x86_mmx %444)
  %446 = bitcast x86_mmx %445 to <2 x i32>
  store <2 x i32> %446, <2 x i32>* %tmp_V2i, align 8
  %447 = load <8 x i16>* %tmp_V8s, align 16
  %448 = load <8 x i16>* %tmp_V8s, align 16
  %449 = call <8 x i16> @llvm.x86.ssse3.phsub.sw.128(<8 x i16> %447, <8 x i16> %448)
  store <8 x i16> %449, <8 x i16>* %tmp_V8s, align 16
  %450 = load <4 x i16>* %tmp_V4s, align 8
  %451 = bitcast <4 x i16> %450 to x86_mmx
  %452 = load <4 x i16>* %tmp_V4s, align 8
  %453 = bitcast <4 x i16> %452 to x86_mmx
  %454 = call x86_mmx @llvm.x86.ssse3.phsub.sw(x86_mmx %451, x86_mmx %453)
  %455 = bitcast x86_mmx %454 to <4 x i16>
  store <4 x i16> %455, <4 x i16>* %tmp_V4s, align 8
  %456 = load <16 x i8>* %tmp_V16c, align 16
  %457 = load <16 x i8>* %tmp_V16c, align 16
  %458 = call <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8> %456, <16 x i8> %457)
  %459 = bitcast <8 x i16> %458 to <16 x i8>
  store <16 x i8> %459, <16 x i8>* %tmp_V16c, align 16
  %460 = load <8 x i8>* %tmp_V8c, align 8
  %461 = bitcast <8 x i8> %460 to x86_mmx
  %462 = load <8 x i8>* %tmp_V8c, align 8
  %463 = bitcast <8 x i8> %462 to x86_mmx
  %464 = call x86_mmx @llvm.x86.ssse3.pmadd.ub.sw(x86_mmx %461, x86_mmx %463)
  %465 = bitcast x86_mmx %464 to <8 x i8>
  store <8 x i8> %465, <8 x i8>* %tmp_V8c, align 8
  %466 = load <8 x i16>* %tmp_V8s, align 16
  %467 = load <8 x i16>* %tmp_V8s, align 16
  %468 = call <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16> %466, <8 x i16> %467)
  store <8 x i16> %468, <8 x i16>* %tmp_V8s, align 16
  %469 = load <4 x i16>* %tmp_V4s, align 8
  %470 = bitcast <4 x i16> %469 to x86_mmx
  %471 = load <4 x i16>* %tmp_V4s, align 8
  %472 = bitcast <4 x i16> %471 to x86_mmx
  %473 = call x86_mmx @llvm.x86.ssse3.pmul.hr.sw(x86_mmx %470, x86_mmx %472)
  %474 = bitcast x86_mmx %473 to <4 x i16>
  store <4 x i16> %474, <4 x i16>* %tmp_V4s, align 8
  %475 = load <16 x i8>* %tmp_V16c, align 16
  %476 = load <16 x i8>* %tmp_V16c, align 16
  %477 = call <16 x i8> @llvm.x86.ssse3.pshuf.b.128(<16 x i8> %475, <16 x i8> %476)
  store <16 x i8> %477, <16 x i8>* %tmp_V16c, align 16
  %478 = load <8 x i8>* %tmp_V8c, align 8
  %479 = bitcast <8 x i8> %478 to x86_mmx
  %480 = load <8 x i8>* %tmp_V8c, align 8
  %481 = bitcast <8 x i8> %480 to x86_mmx
  %482 = call x86_mmx @llvm.x86.ssse3.pshuf.b(x86_mmx %479, x86_mmx %481)
  %483 = bitcast x86_mmx %482 to <8 x i8>
  store <8 x i8> %483, <8 x i8>* %tmp_V8c, align 8
  %484 = load <16 x i8>* %tmp_V16c, align 16
  %485 = load <16 x i8>* %tmp_V16c, align 16
  %486 = call <16 x i8> @llvm.x86.ssse3.psign.b.128(<16 x i8> %484, <16 x i8> %485)
  store <16 x i8> %486, <16 x i8>* %tmp_V16c, align 16
  %487 = load <8 x i8>* %tmp_V8c, align 8
  %488 = bitcast <8 x i8> %487 to x86_mmx
  %489 = load <8 x i8>* %tmp_V8c, align 8
  %490 = bitcast <8 x i8> %489 to x86_mmx
  %491 = call x86_mmx @llvm.x86.ssse3.psign.b(x86_mmx %488, x86_mmx %490)
  %492 = bitcast x86_mmx %491 to <8 x i8>
  store <8 x i8> %492, <8 x i8>* %tmp_V8c, align 8
  %493 = load <8 x i16>* %tmp_V8s, align 16
  %494 = load <8 x i16>* %tmp_V8s, align 16
  %495 = call <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16> %493, <8 x i16> %494)
  store <8 x i16> %495, <8 x i16>* %tmp_V8s, align 16
  %496 = load <4 x i16>* %tmp_V4s, align 8
  %497 = bitcast <4 x i16> %496 to x86_mmx
  %498 = load <4 x i16>* %tmp_V4s, align 8
  %499 = bitcast <4 x i16> %498 to x86_mmx
  %500 = call x86_mmx @llvm.x86.ssse3.psign.w(x86_mmx %497, x86_mmx %499)
  %501 = bitcast x86_mmx %500 to <4 x i16>
  store <4 x i16> %501, <4 x i16>* %tmp_V4s, align 8
  %502 = load <4 x i32>* %tmp_V4i, align 16
  %503 = load <4 x i32>* %tmp_V4i, align 16
  %504 = call <4 x i32> @llvm.x86.ssse3.psign.d.128(<4 x i32> %502, <4 x i32> %503)
  store <4 x i32> %504, <4 x i32>* %tmp_V4i, align 16
  %505 = load <2 x i32>* %tmp_V2i, align 8
  %506 = bitcast <2 x i32> %505 to x86_mmx
  %507 = load <2 x i32>* %tmp_V2i, align 8
  %508 = bitcast <2 x i32> %507 to x86_mmx
  %509 = call x86_mmx @llvm.x86.ssse3.psign.d(x86_mmx %506, x86_mmx %508)
  %510 = bitcast x86_mmx %509 to <2 x i32>
  store <2 x i32> %510, <2 x i32>* %tmp_V2i, align 8
  %511 = load <16 x i8>* %tmp_V16c, align 16
  %512 = call <16 x i8> @llvm.x86.ssse3.pabs.b.128(<16 x i8> %511)
  store <16 x i8> %512, <16 x i8>* %tmp_V16c, align 16
  %513 = load <8 x i8>* %tmp_V8c, align 8
  %514 = bitcast <8 x i8> %513 to x86_mmx
  %515 = call x86_mmx @llvm.x86.ssse3.pabs.b(x86_mmx %514)
  %516 = bitcast x86_mmx %515 to <8 x i8>
  store <8 x i8> %516, <8 x i8>* %tmp_V8c, align 8
  %517 = load <8 x i16>* %tmp_V8s, align 16
  %518 = call <8 x i16> @llvm.x86.ssse3.pabs.w.128(<8 x i16> %517)
  store <8 x i16> %518, <8 x i16>* %tmp_V8s, align 16
  %519 = load <4 x i16>* %tmp_V4s, align 8
  %520 = bitcast <4 x i16> %519 to x86_mmx
  %521 = call x86_mmx @llvm.x86.ssse3.pabs.w(x86_mmx %520)
  %522 = bitcast x86_mmx %521 to <4 x i16>
  store <4 x i16> %522, <4 x i16>* %tmp_V4s, align 8
  %523 = load <4 x i32>* %tmp_V4i, align 16
  %524 = call <4 x i32> @llvm.x86.ssse3.pabs.d.128(<4 x i32> %523)
  store <4 x i32> %524, <4 x i32>* %tmp_V4i, align 16
  %525 = load <2 x i32>* %tmp_V2i, align 8
  %526 = bitcast <2 x i32> %525 to x86_mmx
  %527 = call x86_mmx @llvm.x86.ssse3.pabs.d(x86_mmx %526)
  %528 = bitcast x86_mmx %527 to <2 x i32>
  store <2 x i32> %528, <2 x i32>* %tmp_V2i, align 8
  %529 = load <4 x i16>* %tmp_V4s, align 8
  %530 = bitcast <4 x i16> %529 to x86_mmx
  %531 = load <1 x i64>* %tmp_V1LLi, align 8
  %532 = bitcast <1 x i64> %531 to x86_mmx
  %533 = call x86_mmx @llvm.x86.mmx.psll.w(x86_mmx %530, x86_mmx %532)
  %534 = bitcast x86_mmx %533 to <4 x i16>
  store <4 x i16> %534, <4 x i16>* %tmp_V4s, align 8
  %535 = load <2 x i32>* %tmp_V2i, align 8
  %536 = bitcast <2 x i32> %535 to x86_mmx
  %537 = load <1 x i64>* %tmp_V1LLi, align 8
  %538 = bitcast <1 x i64> %537 to x86_mmx
  %539 = call x86_mmx @llvm.x86.mmx.psll.d(x86_mmx %536, x86_mmx %538)
  %540 = bitcast x86_mmx %539 to <2 x i32>
  store <2 x i32> %540, <2 x i32>* %tmp_V2i, align 8
  %541 = load <1 x i64>* %tmp_V1LLi, align 8
  %542 = bitcast <1 x i64> %541 to x86_mmx
  %543 = load <1 x i64>* %tmp_V1LLi, align 8
  %544 = bitcast <1 x i64> %543 to x86_mmx
  %545 = call x86_mmx @llvm.x86.mmx.psll.q(x86_mmx %542, x86_mmx %544)
  %546 = bitcast x86_mmx %545 to <1 x i64>
  store <1 x i64> %546, <1 x i64>* %tmp_V1LLi, align 8
  %547 = load <4 x i16>* %tmp_V4s, align 8
  %548 = bitcast <4 x i16> %547 to x86_mmx
  %549 = load <1 x i64>* %tmp_V1LLi, align 8
  %550 = bitcast <1 x i64> %549 to x86_mmx
  %551 = call x86_mmx @llvm.x86.mmx.psrl.w(x86_mmx %548, x86_mmx %550)
  %552 = bitcast x86_mmx %551 to <4 x i16>
  store <4 x i16> %552, <4 x i16>* %tmp_V4s, align 8
  %553 = load <2 x i32>* %tmp_V2i, align 8
  %554 = bitcast <2 x i32> %553 to x86_mmx
  %555 = load <1 x i64>* %tmp_V1LLi, align 8
  %556 = bitcast <1 x i64> %555 to x86_mmx
  %557 = call x86_mmx @llvm.x86.mmx.psrl.d(x86_mmx %554, x86_mmx %556)
  %558 = bitcast x86_mmx %557 to <2 x i32>
  store <2 x i32> %558, <2 x i32>* %tmp_V2i, align 8
  %559 = load <1 x i64>* %tmp_V1LLi, align 8
  %560 = bitcast <1 x i64> %559 to x86_mmx
  %561 = load <1 x i64>* %tmp_V1LLi, align 8
  %562 = bitcast <1 x i64> %561 to x86_mmx
  %563 = call x86_mmx @llvm.x86.mmx.psrl.q(x86_mmx %560, x86_mmx %562)
  %564 = bitcast x86_mmx %563 to <1 x i64>
  store <1 x i64> %564, <1 x i64>* %tmp_V1LLi, align 8
  %565 = load <4 x i16>* %tmp_V4s, align 8
  %566 = bitcast <4 x i16> %565 to x86_mmx
  %567 = load <1 x i64>* %tmp_V1LLi, align 8
  %568 = bitcast <1 x i64> %567 to x86_mmx
  %569 = call x86_mmx @llvm.x86.mmx.psra.w(x86_mmx %566, x86_mmx %568)
  %570 = bitcast x86_mmx %569 to <4 x i16>
  store <4 x i16> %570, <4 x i16>* %tmp_V4s, align 8
  %571 = load <2 x i32>* %tmp_V2i, align 8
  %572 = bitcast <2 x i32> %571 to x86_mmx
  %573 = load <1 x i64>* %tmp_V1LLi, align 8
  %574 = bitcast <1 x i64> %573 to x86_mmx
  %575 = call x86_mmx @llvm.x86.mmx.psra.d(x86_mmx %572, x86_mmx %574)
  %576 = bitcast x86_mmx %575 to <2 x i32>
  store <2 x i32> %576, <2 x i32>* %tmp_V2i, align 8
  %577 = load <4 x i16>* %tmp_V4s, align 8
  %578 = bitcast <4 x i16> %577 to x86_mmx
  %579 = load <4 x i16>* %tmp_V4s, align 8
  %580 = bitcast <4 x i16> %579 to x86_mmx
  %581 = call x86_mmx @llvm.x86.mmx.pmadd.wd(x86_mmx %578, x86_mmx %580)
  %582 = bitcast x86_mmx %581 to <2 x i32>
  store <2 x i32> %582, <2 x i32>* %tmp_V2i, align 8
  %583 = load <4 x i16>* %tmp_V4s, align 8
  %584 = bitcast <4 x i16> %583 to x86_mmx
  %585 = load <4 x i16>* %tmp_V4s, align 8
  %586 = bitcast <4 x i16> %585 to x86_mmx
  %587 = call x86_mmx @llvm.x86.mmx.packsswb(x86_mmx %584, x86_mmx %586)
  %588 = bitcast x86_mmx %587 to <8 x i8>
  store <8 x i8> %588, <8 x i8>* %tmp_V8c, align 8
  %589 = load <2 x i32>* %tmp_V2i, align 8
  %590 = bitcast <2 x i32> %589 to x86_mmx
  %591 = load <2 x i32>* %tmp_V2i, align 8
  %592 = bitcast <2 x i32> %591 to x86_mmx
  %593 = call x86_mmx @llvm.x86.mmx.packssdw(x86_mmx %590, x86_mmx %592)
  %594 = bitcast x86_mmx %593 to <4 x i16>
  store <4 x i16> %594, <4 x i16>* %tmp_V4s, align 8
  %595 = load <4 x i16>* %tmp_V4s, align 8
  %596 = bitcast <4 x i16> %595 to x86_mmx
  %597 = load <4 x i16>* %tmp_V4s, align 8
  %598 = bitcast <4 x i16> %597 to x86_mmx
  %599 = call x86_mmx @llvm.x86.mmx.packuswb(x86_mmx %596, x86_mmx %598)
  %600 = bitcast x86_mmx %599 to <8 x i8>
  store <8 x i8> %600, <8 x i8>* %tmp_V8c, align 8
  %601 = load <2 x i32>* %tmp_V2i, align 8
  %602 = extractelement <2 x i32> %601, i32 0
  store i32 %602, i32* %tmp_i, align 4
  %603 = load i32* %tmp_Ui, align 4
  %604 = alloca i32
  store i32 %603, i32* %604
  %605 = bitcast i32* %604 to i8*
  call void @llvm.x86.sse.ldmxcsr(i8* %605)
  %606 = alloca i32
  %607 = bitcast i32* %606 to i8*
  call void @llvm.x86.sse.stmxcsr(i8* %607)
  %stmxcsr = load i32* %606
  store i32 %stmxcsr, i32* %tmp_Ui, align 4
  %608 = load <4 x float>* %tmp_V4f, align 16
  %609 = load <2 x i32>* %tmp_V2i, align 8
  %610 = bitcast <2 x i32> %609 to x86_mmx
  %611 = call <4 x float> @llvm.x86.sse.cvtpi2ps(<4 x float> %608, x86_mmx %610)
  store <4 x float> %611, <4 x float>* %tmp_V4f, align 16
  %612 = load <4 x float>* %tmp_V4f, align 16
  %613 = call x86_mmx @llvm.x86.sse.cvtps2pi(<4 x float> %612)
  %614 = bitcast x86_mmx %613 to <2 x i32>
  store <2 x i32> %614, <2 x i32>* %tmp_V2i, align 8
  %615 = load <4 x float>* %tmp_V4f, align 16
  %616 = call i32 @llvm.x86.sse.cvtss2si(<4 x float> %615)
  store i32 %616, i32* %tmp_i, align 4
  %617 = load <4 x float>* %tmp_V4f, align 16
  %618 = call i64 @llvm.x86.sse.cvtss2si64(<4 x float> %617)
  store i64 %618, i64* %tmp_LLi, align 8
  %619 = load <4 x float>* %tmp_V4f, align 16
  %620 = call x86_mmx @llvm.x86.sse.cvttps2pi(<4 x float> %619)
  %621 = bitcast x86_mmx %620 to <2 x i32>
  store <2 x i32> %621, <2 x i32>* %tmp_V2i, align 8
  %622 = load <8 x i8>* %tmp_V8c, align 8
  %623 = bitcast <8 x i8> %622 to x86_mmx
  %624 = load <8 x i8>* %tmp_V8c, align 8
  %625 = bitcast <8 x i8> %624 to x86_mmx
  %626 = load i8** %tmp_cp, align 8
  call void @llvm.x86.mmx.maskmovq(x86_mmx %623, x86_mmx %625, i8* %626)
  %627 = load float** %tmp_fp, align 8
  %628 = bitcast float* %627 to i8*
  %629 = load <4 x float>* %tmp_V4f, align 16
  call void @llvm.x86.sse.storeu.ps(i8* %628, <4 x float> %629)
  %630 = load <2 x i32>** %tmp_V2ip, align 8
  %631 = load <4 x float>* %tmp_V4f, align 16
  %cast = bitcast <4 x float> %631 to <2 x i64>
  %extract = extractelement <2 x i64> %cast, i32 1
  %632 = bitcast <2 x i32>* %630 to i64*
  store i64 %extract, i64* %632
  %633 = load <2 x i32>** %tmp_V2ip, align 8
  %634 = load <4 x float>* %tmp_V4f, align 16
  %cast1 = bitcast <4 x float> %634 to <2 x i64>
  %extract2 = extractelement <2 x i64> %cast1, i32 0
  %635 = bitcast <2 x i32>* %633 to i64*
  store i64 %extract2, i64* %635
  %636 = load <4 x float>* %tmp_V4f, align 16
  %637 = call i32 @llvm.x86.sse.movmsk.ps(<4 x float> %636)
  store i32 %637, i32* %tmp_i, align 4
  %638 = load <8 x i8>* %tmp_V8c, align 8
  %639 = bitcast <8 x i8> %638 to x86_mmx
  %640 = call i32 @llvm.x86.mmx.pmovmskb(x86_mmx %639)
  store i32 %640, i32* %tmp_i, align 4
  %641 = load float** %tmp_fp, align 8
  %642 = load <4 x float>* %tmp_V4f, align 16
  %cast3 = bitcast float* %641 to <4 x float>*
  store <4 x float> %642, <4 x float>* %cast3, align 16, !nontemporal !0
  %643 = load <1 x i64>** %tmp_V1LLip, align 8
  %644 = bitcast <1 x i64>* %643 to x86_mmx*
  %645 = load <1 x i64>* %tmp_V1LLi, align 8
  %646 = bitcast <1 x i64> %645 to x86_mmx
  call void @llvm.x86.mmx.movnt.dq(x86_mmx* %644, x86_mmx %646)
  call void @llvm.x86.sse.sfence()
  %647 = load <8 x i8>* %tmp_V8c, align 8
  %648 = bitcast <8 x i8> %647 to x86_mmx
  %649 = load <8 x i8>* %tmp_V8c, align 8
  %650 = bitcast <8 x i8> %649 to x86_mmx
  %651 = call x86_mmx @llvm.x86.mmx.psad.bw(x86_mmx %648, x86_mmx %650)
  %652 = bitcast x86_mmx %651 to <4 x i16>
  store <4 x i16> %652, <4 x i16>* %tmp_V4s, align 8
  %653 = load <4 x float>* %tmp_V4f, align 16
  %654 = call <4 x float> @llvm.x86.sse.rcp.ps(<4 x float> %653)
  store <4 x float> %654, <4 x float>* %tmp_V4f, align 16
  %655 = load <4 x float>* %tmp_V4f, align 16
  %656 = call <4 x float> @llvm.x86.sse.rcp.ss(<4 x float> %655)
  store <4 x float> %656, <4 x float>* %tmp_V4f, align 16
  %657 = load <4 x float>* %tmp_V4f, align 16
  %658 = call <4 x float> @llvm.x86.sse.rsqrt.ps(<4 x float> %657)
  store <4 x float> %658, <4 x float>* %tmp_V4f, align 16
  %659 = load <4 x float>* %tmp_V4f, align 16
  %660 = call <4 x float> @llvm.x86.sse.rsqrt.ss(<4 x float> %659)
  store <4 x float> %660, <4 x float>* %tmp_V4f, align 16
  %661 = load <4 x float>* %tmp_V4f, align 16
  %662 = call <4 x float> @llvm.x86.sse.sqrt.ps(<4 x float> %661)
  store <4 x float> %662, <4 x float>* %tmp_V4f, align 16
  %663 = load <4 x float>* %tmp_V4f, align 16
  %664 = call <4 x float> @llvm.x86.sse.sqrt.ss(<4 x float> %663)
  store <4 x float> %664, <4 x float>* %tmp_V4f, align 16
  %665 = load <16 x i8>* %tmp_V16c, align 16
  %666 = load <16 x i8>* %tmp_V16c, align 16
  %667 = load i8** %tmp_cp, align 8
  call void @llvm.x86.sse2.maskmov.dqu(<16 x i8> %665, <16 x i8> %666, i8* %667)
  %668 = load double** %tmp_dp, align 8
  %669 = bitcast double* %668 to i8*
  %670 = load <2 x double>* %tmp_V2d, align 16
  call void @llvm.x86.sse2.storeu.pd(i8* %669, <2 x double> %670)
  %671 = load <2 x double>* %tmp_V2d, align 16
  %672 = call i32 @llvm.x86.sse2.movmsk.pd(<2 x double> %671)
  store i32 %672, i32* %tmp_i, align 4
  %673 = load <16 x i8>* %tmp_V16c, align 16
  %674 = call i32 @llvm.x86.sse2.pmovmskb.128(<16 x i8> %673)
  store i32 %674, i32* %tmp_i, align 4
  %675 = load i32** %tmp_ip, align 8
  %676 = load i32* %tmp_i, align 4
  store i32 %676, i32* %675, align 16, !nontemporal !0
  %677 = load double** %tmp_dp, align 8
  %678 = load <2 x double>* %tmp_V2d, align 16
  %cast4 = bitcast double* %677 to <2 x double>*
  store <2 x double> %678, <2 x double>* %cast4, align 16, !nontemporal !0
  %679 = load <2 x i64>** %tmp_V2LLip, align 8
  %680 = load <2 x i64>* %tmp_V2LLi, align 16
  store <2 x i64> %680, <2 x i64>* %679, align 16, !nontemporal !0
  %681 = load <16 x i8>* %tmp_V16c, align 16
  %682 = load <16 x i8>* %tmp_V16c, align 16
  %683 = call <2 x i64> @llvm.x86.sse2.psad.bw(<16 x i8> %681, <16 x i8> %682)
  store <2 x i64> %683, <2 x i64>* %tmp_V2LLi, align 16
  %684 = load <2 x double>* %tmp_V2d, align 16
  %685 = call <2 x double> @llvm.x86.sse2.sqrt.pd(<2 x double> %684)
  store <2 x double> %685, <2 x double>* %tmp_V2d, align 16
  %686 = load <2 x double>* %tmp_V2d, align 16
  %687 = call <2 x double> @llvm.x86.sse2.sqrt.sd(<2 x double> %686)
  store <2 x double> %687, <2 x double>* %tmp_V2d, align 16
  %688 = load <4 x i32>* %tmp_V4i, align 16
  %689 = call <2 x double> @llvm.x86.sse2.cvtdq2pd(<4 x i32> %688)
  store <2 x double> %689, <2 x double>* %tmp_V2d, align 16
  %690 = load <4 x i32>* %tmp_V4i, align 16
  %691 = call <4 x float> @llvm.x86.sse2.cvtdq2ps(<4 x i32> %690)
  store <4 x float> %691, <4 x float>* %tmp_V4f, align 16
  %692 = load <2 x double>* %tmp_V2d, align 16
  %693 = call <4 x i32> @llvm.x86.sse2.cvtpd2dq(<2 x double> %692)
  %694 = bitcast <4 x i32> %693 to <2 x i64>
  store <2 x i64> %694, <2 x i64>* %tmp_V2LLi, align 16
  %695 = load <2 x double>* %tmp_V2d, align 16
  %696 = call x86_mmx @llvm.x86.sse.cvtpd2pi(<2 x double> %695)
  %697 = bitcast x86_mmx %696 to <2 x i32>
  store <2 x i32> %697, <2 x i32>* %tmp_V2i, align 8
  %698 = load <2 x double>* %tmp_V2d, align 16
  %699 = call <4 x float> @llvm.x86.sse2.cvtpd2ps(<2 x double> %698)
  store <4 x float> %699, <4 x float>* %tmp_V4f, align 16
  %700 = load <2 x double>* %tmp_V2d, align 16
  %701 = call <4 x i32> @llvm.x86.sse2.cvttpd2dq(<2 x double> %700)
  store <4 x i32> %701, <4 x i32>* %tmp_V4i, align 16
  %702 = load <2 x double>* %tmp_V2d, align 16
  %703 = call x86_mmx @llvm.x86.sse.cvttpd2pi(<2 x double> %702)
  %704 = bitcast x86_mmx %703 to <2 x i32>
  store <2 x i32> %704, <2 x i32>* %tmp_V2i, align 8
  %705 = load <2 x i32>* %tmp_V2i, align 8
  %706 = bitcast <2 x i32> %705 to x86_mmx
  %707 = call <2 x double> @llvm.x86.sse.cvtpi2pd(x86_mmx %706)
  store <2 x double> %707, <2 x double>* %tmp_V2d, align 16
  %708 = load <2 x double>* %tmp_V2d, align 16
  %709 = call i32 @llvm.x86.sse2.cvtsd2si(<2 x double> %708)
  store i32 %709, i32* %tmp_i, align 4
  %710 = load <2 x double>* %tmp_V2d, align 16
  %711 = call i64 @llvm.x86.sse2.cvtsd2si64(<2 x double> %710)
  store i64 %711, i64* %tmp_LLi, align 8
  %712 = load <4 x float>* %tmp_V4f, align 16
  %713 = call <4 x i32> @llvm.x86.sse2.cvtps2dq(<4 x float> %712)
  store <4 x i32> %713, <4 x i32>* %tmp_V4i, align 16
  %714 = load <4 x float>* %tmp_V4f, align 16
  %715 = call <2 x double> @llvm.x86.sse2.cvtps2pd(<4 x float> %714)
  store <2 x double> %715, <2 x double>* %tmp_V2d, align 16
  %716 = load <4 x float>* %tmp_V4f, align 16
  %717 = call <4 x i32> @llvm.x86.sse2.cvttps2dq(<4 x float> %716)
  store <4 x i32> %717, <4 x i32>* %tmp_V4i, align 16
  %718 = load i8** %tmp_vCp, align 8
  call void @llvm.x86.sse2.clflush(i8* %718)
  call void @llvm.x86.sse2.lfence()
  call void @llvm.x86.sse2.mfence()
  %719 = load i8** %tmp_cp, align 8
  %720 = load <16 x i8>* %tmp_V16c, align 16
  call void @llvm.x86.sse2.storeu.dq(i8* %719, <16 x i8> %720)
  %721 = load <4 x i16>* %tmp_V4s, align 8
  %722 = bitcast <4 x i16> %721 to x86_mmx
  %723 = load i32* %tmp_i, align 4
  %724 = call x86_mmx @llvm.x86.mmx.pslli.w(x86_mmx %722, i32 %723)
  %725 = bitcast x86_mmx %724 to <4 x i16>
  store <4 x i16> %725, <4 x i16>* %tmp_V4s, align 8
  %726 = load <2 x i32>* %tmp_V2i, align 8
  %727 = bitcast <2 x i32> %726 to x86_mmx
  %728 = load i32* %tmp_i, align 4
  %729 = call x86_mmx @llvm.x86.mmx.pslli.d(x86_mmx %727, i32 %728)
  %730 = bitcast x86_mmx %729 to <2 x i32>
  store <2 x i32> %730, <2 x i32>* %tmp_V2i, align 8
  %731 = load <1 x i64>* %tmp_V1LLi, align 8
  %732 = bitcast <1 x i64> %731 to x86_mmx
  %733 = load i32* %tmp_i, align 4
  %734 = call x86_mmx @llvm.x86.mmx.pslli.q(x86_mmx %732, i32 %733)
  %735 = bitcast x86_mmx %734 to <1 x i64>
  store <1 x i64> %735, <1 x i64>* %tmp_V1LLi, align 8
  %736 = load <4 x i16>* %tmp_V4s, align 8
  %737 = bitcast <4 x i16> %736 to x86_mmx
  %738 = load i32* %tmp_i, align 4
  %739 = call x86_mmx @llvm.x86.mmx.psrai.w(x86_mmx %737, i32 %738)
  %740 = bitcast x86_mmx %739 to <4 x i16>
  store <4 x i16> %740, <4 x i16>* %tmp_V4s, align 8
  %741 = load <2 x i32>* %tmp_V2i, align 8
  %742 = bitcast <2 x i32> %741 to x86_mmx
  %743 = load i32* %tmp_i, align 4
  %744 = call x86_mmx @llvm.x86.mmx.psrai.d(x86_mmx %742, i32 %743)
  %745 = bitcast x86_mmx %744 to <2 x i32>
  store <2 x i32> %745, <2 x i32>* %tmp_V2i, align 8
  %746 = load <4 x i16>* %tmp_V4s, align 8
  %747 = bitcast <4 x i16> %746 to x86_mmx
  %748 = load i32* %tmp_i, align 4
  %749 = call x86_mmx @llvm.x86.mmx.psrli.w(x86_mmx %747, i32 %748)
  %750 = bitcast x86_mmx %749 to <4 x i16>
  store <4 x i16> %750, <4 x i16>* %tmp_V4s, align 8
  %751 = load <2 x i32>* %tmp_V2i, align 8
  %752 = bitcast <2 x i32> %751 to x86_mmx
  %753 = load i32* %tmp_i, align 4
  %754 = call x86_mmx @llvm.x86.mmx.psrli.d(x86_mmx %752, i32 %753)
  %755 = bitcast x86_mmx %754 to <2 x i32>
  store <2 x i32> %755, <2 x i32>* %tmp_V2i, align 8
  %756 = load <1 x i64>* %tmp_V1LLi, align 8
  %757 = bitcast <1 x i64> %756 to x86_mmx
  %758 = load i32* %tmp_i, align 4
  %759 = call x86_mmx @llvm.x86.mmx.psrli.q(x86_mmx %757, i32 %758)
  %760 = bitcast x86_mmx %759 to <1 x i64>
  store <1 x i64> %760, <1 x i64>* %tmp_V1LLi, align 8
  %761 = load <2 x i32>* %tmp_V2i, align 8
  %762 = bitcast <2 x i32> %761 to x86_mmx
  %763 = load <2 x i32>* %tmp_V2i, align 8
  %764 = bitcast <2 x i32> %763 to x86_mmx
  %765 = call x86_mmx @llvm.x86.mmx.pmulu.dq(x86_mmx %762, x86_mmx %764)
  %766 = bitcast x86_mmx %765 to <1 x i64>
  store <1 x i64> %766, <1 x i64>* %tmp_V1LLi, align 8
  %767 = load <4 x i32>* %tmp_V4i, align 16
  %768 = load <4 x i32>* %tmp_V4i, align 16
  %769 = call <2 x i64> @llvm.x86.sse2.pmulu.dq(<4 x i32> %767, <4 x i32> %768)
  store <2 x i64> %769, <2 x i64>* %tmp_V2LLi, align 16
  %770 = load <8 x i16>* %tmp_V8s, align 16
  %771 = load <8 x i16>* %tmp_V8s, align 16
  %772 = call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> %770, <8 x i16> %771)
  store <8 x i16> %772, <8 x i16>* %tmp_V8s, align 16
  %773 = load <4 x i32>* %tmp_V4i, align 16
  %774 = load <4 x i32>* %tmp_V4i, align 16
  %775 = call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> %773, <4 x i32> %774)
  store <4 x i32> %775, <4 x i32>* %tmp_V4i, align 16
  %776 = load <8 x i16>* %tmp_V8s, align 16
  %777 = load <8 x i16>* %tmp_V8s, align 16
  %778 = call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> %776, <8 x i16> %777)
  store <8 x i16> %778, <8 x i16>* %tmp_V8s, align 16
  %779 = load <4 x i32>* %tmp_V4i, align 16
  %780 = load <4 x i32>* %tmp_V4i, align 16
  %781 = call <4 x i32> @llvm.x86.sse2.psrl.d(<4 x i32> %779, <4 x i32> %780)
  store <4 x i32> %781, <4 x i32>* %tmp_V4i, align 16
  %782 = load <2 x i64>* %tmp_V2LLi, align 16
  %783 = load <2 x i64>* %tmp_V2LLi, align 16
  %784 = call <2 x i64> @llvm.x86.sse2.psrl.q(<2 x i64> %782, <2 x i64> %783)
  store <2 x i64> %784, <2 x i64>* %tmp_V2LLi, align 16
  %785 = load <8 x i16>* %tmp_V8s, align 16
  %786 = load <8 x i16>* %tmp_V8s, align 16
  %787 = call <8 x i16> @llvm.x86.sse2.psll.w(<8 x i16> %785, <8 x i16> %786)
  store <8 x i16> %787, <8 x i16>* %tmp_V8s, align 16
  %788 = load <4 x i32>* %tmp_V4i, align 16
  %789 = load <4 x i32>* %tmp_V4i, align 16
  %790 = call <4 x i32> @llvm.x86.sse2.psll.d(<4 x i32> %788, <4 x i32> %789)
  store <4 x i32> %790, <4 x i32>* %tmp_V4i, align 16
  %791 = load <2 x i64>* %tmp_V2LLi, align 16
  %792 = load <2 x i64>* %tmp_V2LLi, align 16
  %793 = call <2 x i64> @llvm.x86.sse2.psll.q(<2 x i64> %791, <2 x i64> %792)
  store <2 x i64> %793, <2 x i64>* %tmp_V2LLi, align 16
  %794 = load <8 x i16>* %tmp_V8s, align 16
  %795 = load i32* %tmp_i, align 4
  %796 = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> %794, i32 %795)
  store <8 x i16> %796, <8 x i16>* %tmp_V8s, align 16
  %797 = load <4 x i32>* %tmp_V4i, align 16
  %798 = load i32* %tmp_i, align 4
  %799 = call <4 x i32> @llvm.x86.sse2.pslli.d(<4 x i32> %797, i32 %798)
  store <4 x i32> %799, <4 x i32>* %tmp_V4i, align 16
  %800 = load <2 x i64>* %tmp_V2LLi, align 16
  %801 = load i32* %tmp_i, align 4
  %802 = call <2 x i64> @llvm.x86.sse2.pslli.q(<2 x i64> %800, i32 %801)
  store <2 x i64> %802, <2 x i64>* %tmp_V2LLi, align 16
  %803 = load <8 x i16>* %tmp_V8s, align 16
  %804 = load i32* %tmp_i, align 4
  %805 = call <8 x i16> @llvm.x86.sse2.psrli.w(<8 x i16> %803, i32 %804)
  store <8 x i16> %805, <8 x i16>* %tmp_V8s, align 16
  %806 = load <4 x i32>* %tmp_V4i, align 16
  %807 = load i32* %tmp_i, align 4
  %808 = call <4 x i32> @llvm.x86.sse2.psrli.d(<4 x i32> %806, i32 %807)
  store <4 x i32> %808, <4 x i32>* %tmp_V4i, align 16
  %809 = load <2 x i64>* %tmp_V2LLi, align 16
  %810 = load i32* %tmp_i, align 4
  %811 = call <2 x i64> @llvm.x86.sse2.psrli.q(<2 x i64> %809, i32 %810)
  store <2 x i64> %811, <2 x i64>* %tmp_V2LLi, align 16
  %812 = load <8 x i16>* %tmp_V8s, align 16
  %813 = load i32* %tmp_i, align 4
  %814 = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> %812, i32 %813)
  store <8 x i16> %814, <8 x i16>* %tmp_V8s, align 16
  %815 = load <4 x i32>* %tmp_V4i, align 16
  %816 = load i32* %tmp_i, align 4
  %817 = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> %815, i32 %816)
  store <4 x i32> %817, <4 x i32>* %tmp_V4i, align 16
  %818 = load <8 x i16>* %tmp_V8s, align 16
  %819 = load <8 x i16>* %tmp_V8s, align 16
  %820 = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> %818, <8 x i16> %819)
  %821 = bitcast <4 x i32> %820 to <8 x i16>
  store <8 x i16> %821, <8 x i16>* %tmp_V8s, align 16
  %822 = load i8** %tmp_vp, align 8
  %823 = load i32* %tmp_Ui, align 4
  %824 = load i32* %tmp_Ui, align 4
  call void @llvm.x86.sse3.monitor(i8* %822, i32 %823, i32 %824)
  %825 = load i32* %tmp_Ui, align 4
  %826 = load i32* %tmp_Ui, align 4
  call void @llvm.x86.sse3.mwait(i32 %825, i32 %826)
  %827 = load i8** %tmp_cCp, align 8
  %828 = call <16 x i8> @llvm.x86.sse3.ldu.dq(i8* %827)
  store <16 x i8> %828, <16 x i8>* %tmp_V16c, align 16
  %829 = load <2 x i64>* %tmp_V2LLi, align 16
  %830 = bitcast <2 x i64> %829 to <16 x i8>
  %831 = load <2 x i64>* %tmp_V2LLi, align 16
  %832 = bitcast <2 x i64> %831 to <16 x i8>
  store <2 x i64> zeroinitializer, <2 x i64>* %tmp_V2LLi, align 16
  %833 = load <1 x i64>* %tmp_V1LLi, align 8
  %834 = bitcast <1 x i64> %833 to <8 x i8>
  %835 = bitcast <8 x i8> %834 to x86_mmx
  %836 = load <1 x i64>* %tmp_V1LLi, align 8
  %837 = bitcast <1 x i64> %836 to <8 x i8>
  %838 = bitcast <8 x i8> %837 to x86_mmx
  %839 = call x86_mmx @llvm.x86.mmx.palignr.b(x86_mmx %835, x86_mmx %838, i8 32)
  %840 = bitcast x86_mmx %839 to <8 x i8>
  %841 = bitcast <8 x i8> %840 to <1 x i64>
  store <1 x i64> %841, <1 x i64>* %tmp_V1LLi, align 8
  %842 = load <4 x double>* %tmp_V4d, align 32
  %843 = load <4 x double>* %tmp_V4d, align 32
  %844 = call <4 x double> @llvm.x86.avx.addsub.pd.256(<4 x double> %842, <4 x double> %843)
  store <4 x double> %844, <4 x double>* %tmp_V4d, align 32
  %845 = load <8 x float>* %tmp_V8f, align 32
  %846 = load <8 x float>* %tmp_V8f, align 32
  %847 = call <8 x float> @llvm.x86.avx.addsub.ps.256(<8 x float> %845, <8 x float> %846)
  store <8 x float> %847, <8 x float>* %tmp_V8f, align 32
  %848 = load <4 x double>* %tmp_V4d, align 32
  %849 = load <4 x double>* %tmp_V4d, align 32
  %850 = call <4 x double> @llvm.x86.avx.hadd.pd.256(<4 x double> %848, <4 x double> %849)
  store <4 x double> %850, <4 x double>* %tmp_V4d, align 32
  %851 = load <8 x float>* %tmp_V8f, align 32
  %852 = load <8 x float>* %tmp_V8f, align 32
  %853 = call <8 x float> @llvm.x86.avx.hsub.ps.256(<8 x float> %851, <8 x float> %852)
  store <8 x float> %853, <8 x float>* %tmp_V8f, align 32
  %854 = load <4 x double>* %tmp_V4d, align 32
  %855 = load <4 x double>* %tmp_V4d, align 32
  %856 = call <4 x double> @llvm.x86.avx.hsub.pd.256(<4 x double> %854, <4 x double> %855)
  store <4 x double> %856, <4 x double>* %tmp_V4d, align 32
  %857 = load <8 x float>* %tmp_V8f, align 32
  %858 = load <8 x float>* %tmp_V8f, align 32
  %859 = call <8 x float> @llvm.x86.avx.hadd.ps.256(<8 x float> %857, <8 x float> %858)
  store <8 x float> %859, <8 x float>* %tmp_V8f, align 32
  %860 = load <4 x double>* %tmp_V4d, align 32
  %861 = load <4 x double>* %tmp_V4d, align 32
  %862 = call <4 x double> @llvm.x86.avx.max.pd.256(<4 x double> %860, <4 x double> %861)
  store <4 x double> %862, <4 x double>* %tmp_V4d, align 32
  %863 = load <8 x float>* %tmp_V8f, align 32
  %864 = load <8 x float>* %tmp_V8f, align 32
  %865 = call <8 x float> @llvm.x86.avx.max.ps.256(<8 x float> %863, <8 x float> %864)
  store <8 x float> %865, <8 x float>* %tmp_V8f, align 32
  %866 = load <4 x double>* %tmp_V4d, align 32
  %867 = load <4 x double>* %tmp_V4d, align 32
  %868 = call <4 x double> @llvm.x86.avx.min.pd.256(<4 x double> %866, <4 x double> %867)
  store <4 x double> %868, <4 x double>* %tmp_V4d, align 32
  %869 = load <8 x float>* %tmp_V8f, align 32
  %870 = load <8 x float>* %tmp_V8f, align 32
  %871 = call <8 x float> @llvm.x86.avx.min.ps.256(<8 x float> %869, <8 x float> %870)
  store <8 x float> %871, <8 x float>* %tmp_V8f, align 32
  %872 = load <2 x double>* %tmp_V2d, align 16
  %873 = load <2 x i64>* %tmp_V2LLi, align 16
  %874 = call <2 x double> @llvm.x86.avx.vpermilvar.pd(<2 x double> %872, <2 x i64> %873)
  store <2 x double> %874, <2 x double>* %tmp_V2d, align 16
  %875 = load <4 x float>* %tmp_V4f, align 16
  %876 = load <4 x i32>* %tmp_V4i, align 16
  %877 = call <4 x float> @llvm.x86.avx.vpermilvar.ps(<4 x float> %875, <4 x i32> %876)
  store <4 x float> %877, <4 x float>* %tmp_V4f, align 16
  %878 = load <4 x double>* %tmp_V4d, align 32
  %879 = load <4 x i64>* %tmp_V4LLi, align 32
  %880 = call <4 x double> @llvm.x86.avx.vpermilvar.pd.256(<4 x double> %878, <4 x i64> %879)
  store <4 x double> %880, <4 x double>* %tmp_V4d, align 32
  %881 = load <8 x float>* %tmp_V8f, align 32
  %882 = load <8 x i32>* %tmp_V8i, align 32
  %883 = call <8 x float> @llvm.x86.avx.vpermilvar.ps.256(<8 x float> %881, <8 x i32> %882)
  store <8 x float> %883, <8 x float>* %tmp_V8f, align 32
  %884 = load <4 x double>* %tmp_V4d, align 32
  %885 = load <4 x double>* %tmp_V4d, align 32
  %886 = call <4 x double> @llvm.x86.avx.blend.pd.256(<4 x double> %884, <4 x double> %885, i32 7)
  store <4 x double> %886, <4 x double>* %tmp_V4d, align 32
  %887 = load <8 x float>* %tmp_V8f, align 32
  %888 = load <8 x float>* %tmp_V8f, align 32
  %889 = call <8 x float> @llvm.x86.avx.blend.ps.256(<8 x float> %887, <8 x float> %888, i32 7)
  store <8 x float> %889, <8 x float>* %tmp_V8f, align 32
  %890 = load <4 x double>* %tmp_V4d, align 32
  %891 = load <4 x double>* %tmp_V4d, align 32
  %892 = load <4 x double>* %tmp_V4d, align 32
  %893 = call <4 x double> @llvm.x86.avx.blendv.pd.256(<4 x double> %890, <4 x double> %891, <4 x double> %892)
  store <4 x double> %893, <4 x double>* %tmp_V4d, align 32
  %894 = load <8 x float>* %tmp_V8f, align 32
  %895 = load <8 x float>* %tmp_V8f, align 32
  %896 = load <8 x float>* %tmp_V8f, align 32
  %897 = call <8 x float> @llvm.x86.avx.blendv.ps.256(<8 x float> %894, <8 x float> %895, <8 x float> %896)
  store <8 x float> %897, <8 x float>* %tmp_V8f, align 32
  %898 = load <8 x float>* %tmp_V8f, align 32
  %899 = load <8 x float>* %tmp_V8f, align 32
  %900 = call <8 x float> @llvm.x86.avx.dp.ps.256(<8 x float> %898, <8 x float> %899, i32 7)
  store <8 x float> %900, <8 x float>* %tmp_V8f, align 32
  %901 = load <4 x double>* %tmp_V4d, align 32
  %902 = load <4 x double>* %tmp_V4d, align 32
  %903 = call <4 x double> @llvm.x86.avx.cmp.pd.256(<4 x double> %901, <4 x double> %902, i8 0)
  store <4 x double> %903, <4 x double>* %tmp_V4d, align 32
  %904 = load <8 x float>* %tmp_V8f, align 32
  %905 = load <8 x float>* %tmp_V8f, align 32
  %906 = call <8 x float> @llvm.x86.avx.cmp.ps.256(<8 x float> %904, <8 x float> %905, i8 0)
  store <8 x float> %906, <8 x float>* %tmp_V8f, align 32
  %907 = load <4 x double>* %tmp_V4d, align 32
  %908 = call <2 x double> @llvm.x86.avx.vextractf128.pd.256(<4 x double> %907, i8 7)
  store <2 x double> %908, <2 x double>* %tmp_V2d, align 16
  %909 = load <8 x float>* %tmp_V8f, align 32
  %910 = call <4 x float> @llvm.x86.avx.vextractf128.ps.256(<8 x float> %909, i8 7)
  store <4 x float> %910, <4 x float>* %tmp_V4f, align 16
  %911 = load <8 x i32>* %tmp_V8i, align 32
  %912 = call <4 x i32> @llvm.x86.avx.vextractf128.si.256(<8 x i32> %911, i8 7)
  store <4 x i32> %912, <4 x i32>* %tmp_V4i, align 16
  %913 = load <4 x i32>* %tmp_V4i, align 16
  %914 = call <4 x double> @llvm.x86.avx.cvtdq2.pd.256(<4 x i32> %913)
  store <4 x double> %914, <4 x double>* %tmp_V4d, align 32
  %915 = load <8 x i32>* %tmp_V8i, align 32
  %916 = call <8 x float> @llvm.x86.avx.cvtdq2.ps.256(<8 x i32> %915)
  store <8 x float> %916, <8 x float>* %tmp_V8f, align 32
  %917 = load <4 x double>* %tmp_V4d, align 32
  %918 = call <4 x float> @llvm.x86.avx.cvt.pd2.ps.256(<4 x double> %917)
  store <4 x float> %918, <4 x float>* %tmp_V4f, align 16
  %919 = load <8 x float>* %tmp_V8f, align 32
  %920 = call <8 x i32> @llvm.x86.avx.cvt.ps2dq.256(<8 x float> %919)
  store <8 x i32> %920, <8 x i32>* %tmp_V8i, align 32
  %921 = load <4 x float>* %tmp_V4f, align 16
  %922 = call <4 x double> @llvm.x86.avx.cvt.ps2.pd.256(<4 x float> %921)
  store <4 x double> %922, <4 x double>* %tmp_V4d, align 32
  %923 = load <4 x double>* %tmp_V4d, align 32
  %924 = call <4 x i32> @llvm.x86.avx.cvtt.pd2dq.256(<4 x double> %923)
  store <4 x i32> %924, <4 x i32>* %tmp_V4i, align 16
  %925 = load <4 x double>* %tmp_V4d, align 32
  %926 = call <4 x i32> @llvm.x86.avx.cvt.pd2dq.256(<4 x double> %925)
  store <4 x i32> %926, <4 x i32>* %tmp_V4i, align 16
  %927 = load <8 x float>* %tmp_V8f, align 32
  %928 = call <8 x i32> @llvm.x86.avx.cvtt.ps2dq.256(<8 x float> %927)
  store <8 x i32> %928, <8 x i32>* %tmp_V8i, align 32
  %929 = load <4 x double>* %tmp_V4d, align 32
  %930 = load <4 x double>* %tmp_V4d, align 32
  %931 = call <4 x double> @llvm.x86.avx.vperm2f128.pd.256(<4 x double> %929, <4 x double> %930, i8 7)
  store <4 x double> %931, <4 x double>* %tmp_V4d, align 32
  %932 = load <8 x float>* %tmp_V8f, align 32
  %933 = load <8 x float>* %tmp_V8f, align 32
  %934 = call <8 x float> @llvm.x86.avx.vperm2f128.ps.256(<8 x float> %932, <8 x float> %933, i8 7)
  store <8 x float> %934, <8 x float>* %tmp_V8f, align 32
  %935 = load <8 x i32>* %tmp_V8i, align 32
  %936 = load <8 x i32>* %tmp_V8i, align 32
  %937 = call <8 x i32> @llvm.x86.avx.vperm2f128.si.256(<8 x i32> %935, <8 x i32> %936, i8 7)
  store <8 x i32> %937, <8 x i32>* %tmp_V8i, align 32
  %938 = load <4 x double>* %tmp_V4d, align 32
  %939 = load <2 x double>* %tmp_V2d, align 16
  %940 = call <4 x double> @llvm.x86.avx.vinsertf128.pd.256(<4 x double> %938, <2 x double> %939, i8 7)
  store <4 x double> %940, <4 x double>* %tmp_V4d, align 32
  %941 = load <8 x float>* %tmp_V8f, align 32
  %942 = load <4 x float>* %tmp_V4f, align 16
  %943 = call <8 x float> @llvm.x86.avx.vinsertf128.ps.256(<8 x float> %941, <4 x float> %942, i8 7)
  store <8 x float> %943, <8 x float>* %tmp_V8f, align 32
  %944 = load <8 x i32>* %tmp_V8i, align 32
  %945 = load <4 x i32>* %tmp_V4i, align 16
  %946 = call <8 x i32> @llvm.x86.avx.vinsertf128.si.256(<8 x i32> %944, <4 x i32> %945, i8 7)
  store <8 x i32> %946, <8 x i32>* %tmp_V8i, align 32
  %947 = load <4 x double>* %tmp_V4d, align 32
  %948 = call <4 x double> @llvm.x86.avx.sqrt.pd.256(<4 x double> %947)
  store <4 x double> %948, <4 x double>* %tmp_V4d, align 32
  %949 = load <8 x float>* %tmp_V8f, align 32
  %950 = call <8 x float> @llvm.x86.avx.sqrt.ps.256(<8 x float> %949)
  store <8 x float> %950, <8 x float>* %tmp_V8f, align 32
  %951 = load <8 x float>* %tmp_V8f, align 32
  %952 = call <8 x float> @llvm.x86.avx.rsqrt.ps.256(<8 x float> %951)
  store <8 x float> %952, <8 x float>* %tmp_V8f, align 32
  %953 = load <8 x float>* %tmp_V8f, align 32
  %954 = call <8 x float> @llvm.x86.avx.rcp.ps.256(<8 x float> %953)
  store <8 x float> %954, <8 x float>* %tmp_V8f, align 32
  %955 = load <4 x double>* %tmp_V4d, align 32
  %956 = call <4 x double> @llvm.x86.avx.round.pd.256(<4 x double> %955, i32 1)
  store <4 x double> %956, <4 x double>* %tmp_V4d, align 32
  %957 = load <8 x float>* %tmp_V8f, align 32
  %958 = call <8 x float> @llvm.x86.avx.round.ps.256(<8 x float> %957, i32 1)
  store <8 x float> %958, <8 x float>* %tmp_V8f, align 32
  %959 = load <2 x double>* %tmp_V2d, align 16
  %960 = load <2 x double>* %tmp_V2d, align 16
  %961 = call i32 @llvm.x86.avx.vtestz.pd(<2 x double> %959, <2 x double> %960)
  store i32 %961, i32* %tmp_i, align 4
  %962 = load <2 x double>* %tmp_V2d, align 16
  %963 = load <2 x double>* %tmp_V2d, align 16
  %964 = call i32 @llvm.x86.avx.vtestc.pd(<2 x double> %962, <2 x double> %963)
  store i32 %964, i32* %tmp_i, align 4
  %965 = load <2 x double>* %tmp_V2d, align 16
  %966 = load <2 x double>* %tmp_V2d, align 16
  %967 = call i32 @llvm.x86.avx.vtestnzc.pd(<2 x double> %965, <2 x double> %966)
  store i32 %967, i32* %tmp_i, align 4
  %968 = load <4 x float>* %tmp_V4f, align 16
  %969 = load <4 x float>* %tmp_V4f, align 16
  %970 = call i32 @llvm.x86.avx.vtestz.ps(<4 x float> %968, <4 x float> %969)
  store i32 %970, i32* %tmp_i, align 4
  %971 = load <4 x float>* %tmp_V4f, align 16
  %972 = load <4 x float>* %tmp_V4f, align 16
  %973 = call i32 @llvm.x86.avx.vtestc.ps(<4 x float> %971, <4 x float> %972)
  store i32 %973, i32* %tmp_i, align 4
  %974 = load <4 x float>* %tmp_V4f, align 16
  %975 = load <4 x float>* %tmp_V4f, align 16
  %976 = call i32 @llvm.x86.avx.vtestnzc.ps(<4 x float> %974, <4 x float> %975)
  store i32 %976, i32* %tmp_i, align 4
  %977 = load <4 x double>* %tmp_V4d, align 32
  %978 = load <4 x double>* %tmp_V4d, align 32
  %979 = call i32 @llvm.x86.avx.vtestz.pd.256(<4 x double> %977, <4 x double> %978)
  store i32 %979, i32* %tmp_i, align 4
  %980 = load <4 x double>* %tmp_V4d, align 32
  %981 = load <4 x double>* %tmp_V4d, align 32
  %982 = call i32 @llvm.x86.avx.vtestc.pd.256(<4 x double> %980, <4 x double> %981)
  store i32 %982, i32* %tmp_i, align 4
  %983 = load <4 x double>* %tmp_V4d, align 32
  %984 = load <4 x double>* %tmp_V4d, align 32
  %985 = call i32 @llvm.x86.avx.vtestnzc.pd.256(<4 x double> %983, <4 x double> %984)
  store i32 %985, i32* %tmp_i, align 4
  %986 = load <8 x float>* %tmp_V8f, align 32
  %987 = load <8 x float>* %tmp_V8f, align 32
  %988 = call i32 @llvm.x86.avx.vtestz.ps.256(<8 x float> %986, <8 x float> %987)
  store i32 %988, i32* %tmp_i, align 4
  %989 = load <8 x float>* %tmp_V8f, align 32
  %990 = load <8 x float>* %tmp_V8f, align 32
  %991 = call i32 @llvm.x86.avx.vtestc.ps.256(<8 x float> %989, <8 x float> %990)
  store i32 %991, i32* %tmp_i, align 4
  %992 = load <8 x float>* %tmp_V8f, align 32
  %993 = load <8 x float>* %tmp_V8f, align 32
  %994 = call i32 @llvm.x86.avx.vtestnzc.ps.256(<8 x float> %992, <8 x float> %993)
  store i32 %994, i32* %tmp_i, align 4
  %995 = load <4 x i64>* %tmp_V4LLi, align 32
  %996 = load <4 x i64>* %tmp_V4LLi, align 32
  %997 = call i32 @llvm.x86.avx.ptestz.256(<4 x i64> %995, <4 x i64> %996)
  store i32 %997, i32* %tmp_i, align 4
  %998 = load <4 x i64>* %tmp_V4LLi, align 32
  %999 = load <4 x i64>* %tmp_V4LLi, align 32
  %1000 = call i32 @llvm.x86.avx.ptestc.256(<4 x i64> %998, <4 x i64> %999)
  store i32 %1000, i32* %tmp_i, align 4
  %1001 = load <4 x i64>* %tmp_V4LLi, align 32
  %1002 = load <4 x i64>* %tmp_V4LLi, align 32
  %1003 = call i32 @llvm.x86.avx.ptestnzc.256(<4 x i64> %1001, <4 x i64> %1002)
  store i32 %1003, i32* %tmp_i, align 4
  %1004 = load <4 x double>* %tmp_V4d, align 32
  %1005 = call i32 @llvm.x86.avx.movmsk.pd.256(<4 x double> %1004)
  store i32 %1005, i32* %tmp_i, align 4
  %1006 = load <8 x float>* %tmp_V8f, align 32
  %1007 = call i32 @llvm.x86.avx.movmsk.ps.256(<8 x float> %1006)
  store i32 %1007, i32* %tmp_i, align 4
  call void @llvm.x86.avx.vzeroall()
  call void @llvm.x86.avx.vzeroupper()
  %1008 = load float** %tmp_fCp, align 8
  %1009 = bitcast float* %1008 to i8*
  %1010 = call <4 x float> @llvm.x86.avx.vbroadcast.ss(i8* %1009)
  store <4 x float> %1010, <4 x float>* %tmp_V4f, align 16
  %1011 = load double** %tmp_dCp, align 8
  %1012 = bitcast double* %1011 to i8*
  %1013 = call <4 x double> @llvm.x86.avx.vbroadcast.sd.256(i8* %1012)
  store <4 x double> %1013, <4 x double>* %tmp_V4d, align 32
  %1014 = load float** %tmp_fCp, align 8
  %1015 = bitcast float* %1014 to i8*
  %1016 = call <8 x float> @llvm.x86.avx.vbroadcast.ss.256(i8* %1015)
  store <8 x float> %1016, <8 x float>* %tmp_V8f, align 32
  %1017 = load <2 x double>** %tmp_V2dCp, align 8
  %1018 = bitcast <2 x double>* %1017 to i8*
  %1019 = call <4 x double> @llvm.x86.avx.vbroadcastf128.pd.256(i8* %1018)
  store <4 x double> %1019, <4 x double>* %tmp_V4d, align 32
  %1020 = load <4 x float>** %tmp_V4fCp, align 8
  %1021 = bitcast <4 x float>* %1020 to i8*
  %1022 = call <8 x float> @llvm.x86.avx.vbroadcastf128.ps.256(i8* %1021)
  store <8 x float> %1022, <8 x float>* %tmp_V8f, align 32
  %1023 = load double** %tmp_dp, align 8
  %1024 = bitcast double* %1023 to i8*
  %1025 = load <4 x double>* %tmp_V4d, align 32
  call void @llvm.x86.avx.storeu.pd.256(i8* %1024, <4 x double> %1025)
  %1026 = load float** %tmp_fp, align 8
  %1027 = bitcast float* %1026 to i8*
  %1028 = load <8 x float>* %tmp_V8f, align 32
  call void @llvm.x86.avx.storeu.ps.256(i8* %1027, <8 x float> %1028)
  %1029 = load i8** %tmp_cp, align 8
  %1030 = load <32 x i8>* %tmp_V32c, align 32
  call void @llvm.x86.avx.storeu.dq.256(i8* %1029, <32 x i8> %1030)
  %1031 = load i8** %tmp_cCp, align 8
  %1032 = call <32 x i8> @llvm.x86.avx.ldu.dq.256(i8* %1031)
  store <32 x i8> %1032, <32 x i8>* %tmp_V32c, align 32
  %1033 = load <4 x i64>** %tmp_V4LLip, align 8
  %1034 = load <4 x i64>* %tmp_V4LLi, align 32
  store <4 x i64> %1034, <4 x i64>* %1033, align 16, !nontemporal !0
  %1035 = load double** %tmp_dp, align 8
  %1036 = load <4 x double>* %tmp_V4d, align 32
  %cast5 = bitcast double* %1035 to <4 x double>*
  store <4 x double> %1036, <4 x double>* %cast5, align 16, !nontemporal !0
  %1037 = load float** %tmp_fp, align 8
  %1038 = load <8 x float>* %tmp_V8f, align 32
  %cast6 = bitcast float* %1037 to <8 x float>*
  store <8 x float> %1038, <8 x float>* %cast6, align 16, !nontemporal !0
  %1039 = load <2 x double>** %tmp_V2dCp, align 8
  %1040 = bitcast <2 x double>* %1039 to i8*
  %1041 = load <2 x double>* %tmp_V2d, align 16
  %1042 = call <2 x double> @llvm.x86.avx.maskload.pd(i8* %1040, <2 x double> %1041)
  store <2 x double> %1042, <2 x double>* %tmp_V2d, align 16
  %1043 = load <4 x float>** %tmp_V4fCp, align 8
  %1044 = bitcast <4 x float>* %1043 to i8*
  %1045 = load <4 x float>* %tmp_V4f, align 16
  %1046 = call <4 x float> @llvm.x86.avx.maskload.ps(i8* %1044, <4 x float> %1045)
  store <4 x float> %1046, <4 x float>* %tmp_V4f, align 16
  %1047 = load <4 x double>** %tmp_V4dCp, align 8
  %1048 = bitcast <4 x double>* %1047 to i8*
  %1049 = load <4 x double>* %tmp_V4d, align 32
  %1050 = call <4 x double> @llvm.x86.avx.maskload.pd.256(i8* %1048, <4 x double> %1049)
  store <4 x double> %1050, <4 x double>* %tmp_V4d, align 32
  %1051 = load <8 x float>** %tmp_V8fCp, align 8
  %1052 = bitcast <8 x float>* %1051 to i8*
  %1053 = load <8 x float>* %tmp_V8f, align 32
  %1054 = call <8 x float> @llvm.x86.avx.maskload.ps.256(i8* %1052, <8 x float> %1053)
  store <8 x float> %1054, <8 x float>* %tmp_V8f, align 32
  %1055 = load <2 x double>** %tmp_V2dp, align 8
  %1056 = bitcast <2 x double>* %1055 to i8*
  %1057 = load <2 x double>* %tmp_V2d, align 16
  %1058 = load <2 x double>* %tmp_V2d, align 16
  call void @llvm.x86.avx.maskstore.pd(i8* %1056, <2 x double> %1057, <2 x double> %1058)
  %1059 = load <4 x float>** %tmp_V4fp, align 8
  %1060 = bitcast <4 x float>* %1059 to i8*
  %1061 = load <4 x float>* %tmp_V4f, align 16
  %1062 = load <4 x float>* %tmp_V4f, align 16
  call void @llvm.x86.avx.maskstore.ps(i8* %1060, <4 x float> %1061, <4 x float> %1062)
  %1063 = load <4 x double>** %tmp_V4dp, align 8
  %1064 = bitcast <4 x double>* %1063 to i8*
  %1065 = load <4 x double>* %tmp_V4d, align 32
  %1066 = load <4 x double>* %tmp_V4d, align 32
  call void @llvm.x86.avx.maskstore.pd.256(i8* %1064, <4 x double> %1065, <4 x double> %1066)
  %1067 = load <8 x float>** %tmp_V8fp, align 8
  %1068 = bitcast <8 x float>* %1067 to i8*
  %1069 = load <8 x float>* %tmp_V8f, align 32
  %1070 = load <8 x float>* %tmp_V8f, align 32
  call void @llvm.x86.avx.maskstore.ps.256(i8* %1068, <8 x float> %1069, <8 x float> %1070)
  ret void
}

declare i32 @llvm.x86.sse.comieq.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.comilt.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.comile.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.comigt.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.comige.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.comineq.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.ucomieq.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.ucomilt.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.ucomile.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.ucomigt.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.ucomige.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.ucomineq.ss(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.sse2.comieq.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.comilt.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.comile.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.comigt.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.comige.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.comineq.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.ucomieq.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.ucomilt.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.ucomile.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.ucomigt.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.ucomige.sd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.ucomineq.sd(<2 x double>, <2 x double>) nounwind readnone

declare <4 x float> @llvm.x86.sse.cmp.ps(<4 x float>, <4 x float>, i8) nounwind readnone

declare <4 x float> @llvm.x86.sse.cmp.ss(<4 x float>, <4 x float>, i8) nounwind readnone

declare <4 x float> @llvm.x86.sse.min.ps(<4 x float>, <4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.max.ps(<4 x float>, <4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.min.ss(<4 x float>, <4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.max.ss(<4 x float>, <4 x float>) nounwind readnone

declare x86_mmx @llvm.x86.mmx.padds.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.padds.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psubs.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psubs.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.paddus.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.paddus.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psubus.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psubus.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pmulh.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pmulhu.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pavg.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pavg.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pcmpeq.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pcmpeq.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pcmpeq.d(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pcmpgt.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pcmpgt.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pcmpgt.d(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pmaxu.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pmaxs.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pminu.b(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pmins.w(x86_mmx, x86_mmx) nounwind readnone

declare <2 x double> @llvm.x86.sse2.cmp.pd(<2 x double>, <2 x double>, i8) nounwind readnone

declare <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double>, <2 x double>, i8) nounwind readnone

declare <2 x double> @llvm.x86.sse2.min.pd(<2 x double>, <2 x double>) nounwind readnone

declare <2 x double> @llvm.x86.sse2.max.pd(<2 x double>, <2 x double>) nounwind readnone

declare <2 x double> @llvm.x86.sse2.min.sd(<2 x double>, <2 x double>) nounwind readnone

declare <2 x double> @llvm.x86.sse2.max.sd(<2 x double>, <2 x double>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.padds.b(<16 x i8>, <16 x i8>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.padds.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.psubs.b(<16 x i8>, <16 x i8>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.psubs.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.paddus.b(<16 x i8>, <16 x i8>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.paddus.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.psubus.b(<16 x i8>, <16 x i8>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.psubus.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.pmulh.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8>, <16 x i8>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.pavg.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.pmaxu.b(<16 x i8>, <16 x i8>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.pmaxs.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.pminu.b(<16 x i8>, <16 x i8>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.pmins.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16>, <8 x i16>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32>, <4 x i32>) nounwind readnone

declare <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16>, <8 x i16>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <4 x float> @llvm.x86.sse3.addsub.ps(<4 x float>, <4 x float>) nounwind readnone

declare <2 x double> @llvm.x86.sse3.addsub.pd(<2 x double>, <2 x double>) nounwind readnone

declare <4 x float> @llvm.x86.sse3.hadd.ps(<4 x float>, <4 x float>) nounwind readnone

declare <2 x double> @llvm.x86.sse3.hadd.pd(<2 x double>, <2 x double>) nounwind readnone

declare <4 x float> @llvm.x86.sse3.hsub.ps(<4 x float>, <4 x float>) nounwind readnone

declare <2 x double> @llvm.x86.sse3.hsub.pd(<2 x double>, <2 x double>) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.phadd.w.128(<8 x i16>, <8 x i16>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.phadd.w(x86_mmx, x86_mmx) nounwind readnone

declare <4 x i32> @llvm.x86.ssse3.phadd.d.128(<4 x i32>, <4 x i32>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.phadd.d(x86_mmx, x86_mmx) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.phadd.sw.128(<8 x i16>, <8 x i16>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.phadd.sw(x86_mmx, x86_mmx) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.phsub.w.128(<8 x i16>, <8 x i16>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.phsub.w(x86_mmx, x86_mmx) nounwind readnone

declare <4 x i32> @llvm.x86.ssse3.phsub.d.128(<4 x i32>, <4 x i32>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.phsub.d(x86_mmx, x86_mmx) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.phsub.sw.128(<8 x i16>, <8 x i16>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.phsub.sw(x86_mmx, x86_mmx) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.pmadd.ub.sw.128(<16 x i8>, <16 x i8>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.pmadd.ub.sw(x86_mmx, x86_mmx) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.pmul.hr.sw.128(<8 x i16>, <8 x i16>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.pmul.hr.sw(x86_mmx, x86_mmx) nounwind readnone

declare <16 x i8> @llvm.x86.ssse3.pshuf.b.128(<16 x i8>, <16 x i8>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.pshuf.b(x86_mmx, x86_mmx) nounwind readnone

declare <16 x i8> @llvm.x86.ssse3.psign.b.128(<16 x i8>, <16 x i8>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.psign.b(x86_mmx, x86_mmx) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.psign.w.128(<8 x i16>, <8 x i16>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.psign.w(x86_mmx, x86_mmx) nounwind readnone

declare <4 x i32> @llvm.x86.ssse3.psign.d.128(<4 x i32>, <4 x i32>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.psign.d(x86_mmx, x86_mmx) nounwind readnone

declare <16 x i8> @llvm.x86.ssse3.pabs.b.128(<16 x i8>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.pabs.b(x86_mmx) nounwind readnone

declare <8 x i16> @llvm.x86.ssse3.pabs.w.128(<8 x i16>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.pabs.w(x86_mmx) nounwind readnone

declare <4 x i32> @llvm.x86.ssse3.pabs.d.128(<4 x i32>) nounwind readnone

declare x86_mmx @llvm.x86.ssse3.pabs.d(x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psll.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psll.d(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psll.q(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrl.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrl.d(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrl.q(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psra.w(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psra.d(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pmadd.wd(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.packsswb(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.packssdw(x86_mmx, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.mmx.packuswb(x86_mmx, x86_mmx) nounwind readnone

declare void @llvm.x86.sse.ldmxcsr(i8*) nounwind

declare void @llvm.x86.sse.stmxcsr(i8*) nounwind

declare <4 x float> @llvm.x86.sse.cvtpi2ps(<4 x float>, x86_mmx) nounwind readnone

declare x86_mmx @llvm.x86.sse.cvtps2pi(<4 x float>) nounwind readnone

declare i32 @llvm.x86.sse.cvtss2si(<4 x float>) nounwind readnone

declare i64 @llvm.x86.sse.cvtss2si64(<4 x float>) nounwind readnone

declare x86_mmx @llvm.x86.sse.cvttps2pi(<4 x float>) nounwind readnone

declare void @llvm.x86.mmx.maskmovq(x86_mmx, x86_mmx, i8*) nounwind

declare void @llvm.x86.sse.storeu.ps(i8*, <4 x float>) nounwind

declare i32 @llvm.x86.sse.movmsk.ps(<4 x float>) nounwind readnone

declare i32 @llvm.x86.mmx.pmovmskb(x86_mmx) nounwind readnone

declare void @llvm.x86.mmx.movnt.dq(x86_mmx*, x86_mmx) nounwind

declare void @llvm.x86.sse.sfence() nounwind

declare x86_mmx @llvm.x86.mmx.psad.bw(x86_mmx, x86_mmx) nounwind readnone

declare <4 x float> @llvm.x86.sse.rcp.ps(<4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.rcp.ss(<4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.rsqrt.ps(<4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.rsqrt.ss(<4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.sqrt.ps(<4 x float>) nounwind readnone

declare <4 x float> @llvm.x86.sse.sqrt.ss(<4 x float>) nounwind readnone

declare void @llvm.x86.sse2.maskmov.dqu(<16 x i8>, <16 x i8>, i8*) nounwind

declare void @llvm.x86.sse2.storeu.pd(i8*, <2 x double>) nounwind

declare i32 @llvm.x86.sse2.movmsk.pd(<2 x double>) nounwind readnone

declare i32 @llvm.x86.sse2.pmovmskb.128(<16 x i8>) nounwind readnone

declare <2 x i64> @llvm.x86.sse2.psad.bw(<16 x i8>, <16 x i8>) nounwind readnone

declare <2 x double> @llvm.x86.sse2.sqrt.pd(<2 x double>) nounwind readnone

declare <2 x double> @llvm.x86.sse2.sqrt.sd(<2 x double>) nounwind readnone

declare <2 x double> @llvm.x86.sse2.cvtdq2pd(<4 x i32>) nounwind readnone

declare <4 x float> @llvm.x86.sse2.cvtdq2ps(<4 x i32>) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.cvtpd2dq(<2 x double>) nounwind readnone

declare x86_mmx @llvm.x86.sse.cvtpd2pi(<2 x double>) nounwind readnone

declare <4 x float> @llvm.x86.sse2.cvtpd2ps(<2 x double>) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.cvttpd2dq(<2 x double>) nounwind readnone

declare x86_mmx @llvm.x86.sse.cvttpd2pi(<2 x double>) nounwind readnone

declare <2 x double> @llvm.x86.sse.cvtpi2pd(x86_mmx) nounwind readnone

declare i32 @llvm.x86.sse2.cvtsd2si(<2 x double>) nounwind readnone

declare i64 @llvm.x86.sse2.cvtsd2si64(<2 x double>) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.cvtps2dq(<4 x float>) nounwind readnone

declare <2 x double> @llvm.x86.sse2.cvtps2pd(<4 x float>) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.cvttps2dq(<4 x float>) nounwind readnone

declare void @llvm.x86.sse2.clflush(i8*) nounwind

declare void @llvm.x86.sse2.lfence() nounwind

declare void @llvm.x86.sse2.mfence() nounwind

declare void @llvm.x86.sse2.storeu.dq(i8*, <16 x i8>) nounwind

declare x86_mmx @llvm.x86.mmx.pslli.w(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pslli.d(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pslli.q(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrai.w(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrai.d(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrli.w(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrli.d(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.psrli.q(x86_mmx, i32) nounwind readnone

declare x86_mmx @llvm.x86.mmx.pmulu.dq(x86_mmx, x86_mmx) nounwind readnone

declare <2 x i64> @llvm.x86.sse2.pmulu.dq(<4 x i32>, <4 x i32>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32>, <4 x i32>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.psrl.d(<4 x i32>, <4 x i32>) nounwind readnone

declare <2 x i64> @llvm.x86.sse2.psrl.q(<2 x i64>, <2 x i64>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.psll.w(<8 x i16>, <8 x i16>) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.psll.d(<4 x i32>, <4 x i32>) nounwind readnone

declare <2 x i64> @llvm.x86.sse2.psll.q(<2 x i64>, <2 x i64>) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16>, i32) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.pslli.d(<4 x i32>, i32) nounwind readnone

declare <2 x i64> @llvm.x86.sse2.pslli.q(<2 x i64>, i32) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.psrli.w(<8 x i16>, i32) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.psrli.d(<4 x i32>, i32) nounwind readnone

declare <2 x i64> @llvm.x86.sse2.psrli.q(<2 x i64>, i32) nounwind readnone

declare <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16>, i32) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32>, i32) nounwind readnone

declare <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16>, <8 x i16>) nounwind readnone

declare void @llvm.x86.sse3.monitor(i8*, i32, i32) nounwind

declare void @llvm.x86.sse3.mwait(i32, i32) nounwind

declare <16 x i8> @llvm.x86.sse3.ldu.dq(i8*) nounwind readonly

declare x86_mmx @llvm.x86.mmx.palignr.b(x86_mmx, x86_mmx, i8) nounwind readnone

declare <4 x double> @llvm.x86.avx.addsub.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare <8 x float> @llvm.x86.avx.addsub.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare <4 x double> @llvm.x86.avx.hadd.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare <8 x float> @llvm.x86.avx.hsub.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare <4 x double> @llvm.x86.avx.hsub.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare <8 x float> @llvm.x86.avx.hadd.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare <4 x double> @llvm.x86.avx.max.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare <8 x float> @llvm.x86.avx.max.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare <4 x double> @llvm.x86.avx.min.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare <8 x float> @llvm.x86.avx.min.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare <2 x double> @llvm.x86.avx.vpermilvar.pd(<2 x double>, <2 x i64>) nounwind readnone

declare <4 x float> @llvm.x86.avx.vpermilvar.ps(<4 x float>, <4 x i32>) nounwind readnone

declare <4 x double> @llvm.x86.avx.vpermilvar.pd.256(<4 x double>, <4 x i64>) nounwind readnone

declare <8 x float> @llvm.x86.avx.vpermilvar.ps.256(<8 x float>, <8 x i32>) nounwind readnone

declare <4 x double> @llvm.x86.avx.blend.pd.256(<4 x double>, <4 x double>, i32) nounwind readnone

declare <8 x float> @llvm.x86.avx.blend.ps.256(<8 x float>, <8 x float>, i32) nounwind readnone

declare <4 x double> @llvm.x86.avx.blendv.pd.256(<4 x double>, <4 x double>, <4 x double>) nounwind readnone

declare <8 x float> @llvm.x86.avx.blendv.ps.256(<8 x float>, <8 x float>, <8 x float>) nounwind readnone

declare <8 x float> @llvm.x86.avx.dp.ps.256(<8 x float>, <8 x float>, i32) nounwind readnone

declare <4 x double> @llvm.x86.avx.cmp.pd.256(<4 x double>, <4 x double>, i8) nounwind readnone

declare <8 x float> @llvm.x86.avx.cmp.ps.256(<8 x float>, <8 x float>, i8) nounwind readnone

declare <2 x double> @llvm.x86.avx.vextractf128.pd.256(<4 x double>, i8) nounwind readnone

declare <4 x float> @llvm.x86.avx.vextractf128.ps.256(<8 x float>, i8) nounwind readnone

declare <4 x i32> @llvm.x86.avx.vextractf128.si.256(<8 x i32>, i8) nounwind readnone

declare <4 x double> @llvm.x86.avx.cvtdq2.pd.256(<4 x i32>) nounwind readnone

declare <8 x float> @llvm.x86.avx.cvtdq2.ps.256(<8 x i32>) nounwind readnone

declare <4 x float> @llvm.x86.avx.cvt.pd2.ps.256(<4 x double>) nounwind readnone

declare <8 x i32> @llvm.x86.avx.cvt.ps2dq.256(<8 x float>) nounwind readnone

declare <4 x double> @llvm.x86.avx.cvt.ps2.pd.256(<4 x float>) nounwind readnone

declare <4 x i32> @llvm.x86.avx.cvtt.pd2dq.256(<4 x double>) nounwind readnone

declare <4 x i32> @llvm.x86.avx.cvt.pd2dq.256(<4 x double>) nounwind readnone

declare <8 x i32> @llvm.x86.avx.cvtt.ps2dq.256(<8 x float>) nounwind readnone

declare <4 x double> @llvm.x86.avx.vperm2f128.pd.256(<4 x double>, <4 x double>, i8) nounwind readnone

declare <8 x float> @llvm.x86.avx.vperm2f128.ps.256(<8 x float>, <8 x float>, i8) nounwind readnone

declare <8 x i32> @llvm.x86.avx.vperm2f128.si.256(<8 x i32>, <8 x i32>, i8) nounwind readnone

declare <4 x double> @llvm.x86.avx.vinsertf128.pd.256(<4 x double>, <2 x double>, i8) nounwind readnone

declare <8 x float> @llvm.x86.avx.vinsertf128.ps.256(<8 x float>, <4 x float>, i8) nounwind readnone

declare <8 x i32> @llvm.x86.avx.vinsertf128.si.256(<8 x i32>, <4 x i32>, i8) nounwind readnone

declare <4 x double> @llvm.x86.avx.sqrt.pd.256(<4 x double>) nounwind readnone

declare <8 x float> @llvm.x86.avx.sqrt.ps.256(<8 x float>) nounwind readnone

declare <8 x float> @llvm.x86.avx.rsqrt.ps.256(<8 x float>) nounwind readnone

declare <8 x float> @llvm.x86.avx.rcp.ps.256(<8 x float>) nounwind readnone

declare <4 x double> @llvm.x86.avx.round.pd.256(<4 x double>, i32) nounwind readnone

declare <8 x float> @llvm.x86.avx.round.ps.256(<8 x float>, i32) nounwind readnone

declare i32 @llvm.x86.avx.vtestz.pd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.avx.vtestc.pd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.avx.vtestnzc.pd(<2 x double>, <2 x double>) nounwind readnone

declare i32 @llvm.x86.avx.vtestz.ps(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.avx.vtestc.ps(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.avx.vtestnzc.ps(<4 x float>, <4 x float>) nounwind readnone

declare i32 @llvm.x86.avx.vtestz.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare i32 @llvm.x86.avx.vtestc.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare i32 @llvm.x86.avx.vtestnzc.pd.256(<4 x double>, <4 x double>) nounwind readnone

declare i32 @llvm.x86.avx.vtestz.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare i32 @llvm.x86.avx.vtestc.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare i32 @llvm.x86.avx.vtestnzc.ps.256(<8 x float>, <8 x float>) nounwind readnone

declare i32 @llvm.x86.avx.ptestz.256(<4 x i64>, <4 x i64>) nounwind readnone

declare i32 @llvm.x86.avx.ptestc.256(<4 x i64>, <4 x i64>) nounwind readnone

declare i32 @llvm.x86.avx.ptestnzc.256(<4 x i64>, <4 x i64>) nounwind readnone

declare i32 @llvm.x86.avx.movmsk.pd.256(<4 x double>) nounwind readnone

declare i32 @llvm.x86.avx.movmsk.ps.256(<8 x float>) nounwind readnone

declare void @llvm.x86.avx.vzeroall() nounwind

declare void @llvm.x86.avx.vzeroupper() nounwind

declare <4 x float> @llvm.x86.avx.vbroadcast.ss(i8*) nounwind readonly

declare <4 x double> @llvm.x86.avx.vbroadcast.sd.256(i8*) nounwind readonly

declare <8 x float> @llvm.x86.avx.vbroadcast.ss.256(i8*) nounwind readonly

declare <4 x double> @llvm.x86.avx.vbroadcastf128.pd.256(i8*) nounwind readonly

declare <8 x float> @llvm.x86.avx.vbroadcastf128.ps.256(i8*) nounwind readonly

declare void @llvm.x86.avx.storeu.pd.256(i8*, <4 x double>) nounwind

declare void @llvm.x86.avx.storeu.ps.256(i8*, <8 x float>) nounwind

declare void @llvm.x86.avx.storeu.dq.256(i8*, <32 x i8>) nounwind

declare <32 x i8> @llvm.x86.avx.ldu.dq.256(i8*) nounwind readonly

declare <2 x double> @llvm.x86.avx.maskload.pd(i8*, <2 x double>) nounwind readonly

declare <4 x float> @llvm.x86.avx.maskload.ps(i8*, <4 x float>) nounwind readonly

declare <4 x double> @llvm.x86.avx.maskload.pd.256(i8*, <4 x double>) nounwind readonly

declare <8 x float> @llvm.x86.avx.maskload.ps.256(i8*, <8 x float>) nounwind readonly

declare void @llvm.x86.avx.maskstore.pd(i8*, <2 x double>, <2 x double>) nounwind

declare void @llvm.x86.avx.maskstore.ps(i8*, <4 x float>, <4 x float>) nounwind

declare void @llvm.x86.avx.maskstore.pd.256(i8*, <4 x double>, <4 x double>) nounwind

declare void @llvm.x86.avx.maskstore.ps.256(i8*, <8 x float>, <8 x float>) nounwind

!0 = metadata !{i32 1}
